<!DOCTYPE html>
<html lang="en" style="scroll-padding-top: 70px;">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, shrink-to-fit=no">
    <link rel="stylesheet"
          href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic">
    <link href="https://fonts.googleapis.com/css2?family=Exo:wght@400;700&family=Lato:wght@400;700&display=swap" rel="stylesheet">

    <link rel="stylesheet" href="/static/expo/fonts/font-awesome.min.css">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.8.1/css/all.css" integrity="sha384-50oBUHEmvpQ+1lW4y57PTFmhCaXp0ML5d60M1M7uH2+nqUivzIebhndOJK28anvf" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-select@1.13.14/dist/css/bootstrap-select.min.css">
    <link rel="stylesheet" href="cards.css">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.8.1/css/all.css" integrity="sha384-50oBUHEmvpQ+1lW4y57PTFmhCaXp0ML5d60M1M7uH2+nqUivzIebhndOJK28anvf" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" integrity="sha384-xOolHFLEh07PJGoPkLv1IbcEPTNtaed2xpHsD9ESMhqIYd0nLMwNLD69Npy4HI+N" crossorigin="anonymous">

    <script src="https://code.jquery.com/jquery-3.6.1.min.js"
            integrity="sha256-o88AwQnZB+VDvE9tvIXrMQaPlFFSUTR+nldQm1LuPXQ=" crossorigin="anonymous"></script>
    </script>

    <script>
        if (typeof jQuery === 'undefined') {
            var script = document.createElement('script');
            script.type = 'text/javascript';
            script.src = "/static/core/js/jquery-3.6.1.min.js";
            document.head.appendChild(script);
        }
    </script>

    <script src="https://d3js.org/d3.v5.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-Fy6S3B9q64WdZWQUiU+q4/2Lc9npb8tCaSX9FK7E8HnRr0Jz8D6OP9dO5Vg3Q9ct" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap-select@1.13.14/dist/js/bootstrap-select.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/corejs-typeahead/1.3.1/typeahead.bundle.min.js" integrity="sha512-lEb9Vp/rkl9g2E/LdHIMFTqz21+LA79f84gqP75fbimHqVTu6483JG1AwJlWLLQ8ezTehty78fObKupq3HSHPQ==" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/moment@2.24.0/min/moment.min.js"
            integrity="sha256-4iQZ6BVL4qNKlQ27TExEhBN1HFPvAvAMbFavKKosSWQ="
            crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js"></script>
    <script src="/static/core/js/ajax-csrf-snippet.js" type="text/javascript"></script>
    <script src="/static/virtual/js/virtual.js"></script>

    <link rel="stylesheet" href="virtual.css">

    <style>
    body {
        background: #f6f6f6;
    }
    </style>

</head>

<body>
<!-- NAV -->

<!--
<nav class="navbar sticky-top navbar-expand-lg navbar-light mr-auto" id="main-nav">
    <div class="container-fluid">
        <a class="navbar-brand" href="/">
            <img src="/static/core/img/ICML-logo.svg" height="40px">
        </a>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav"
                aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse text-right flex-grow-1" id="navbarNav">
            <ul class="navbar-nav ml-auto">
                <li class="nav-item ">
                    <a class="nav-link" href="/virtual/2022/events/tutorial">Tutorials</a>
                </li>

                <li class="nav-item dropdown">
                    <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button"
                       data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                        Main Conference
                    </a>
                    <div class="dropdown-menu" aria-labelledby="navbarDropdown">
                        <a class="dropdown-item" href="/virtual/2022/events/oral">Orals</a>
                        <div class="dropdown-divider"></div>
                        <a class="dropdown-item" href="/virtual/2022/papers.html">Papers</a>
                    </div>
                </li>
            </ul>
    </div>
    </div>
</nav>
-->

    <!-- NAV -->

    <nav class="navbar sticky-top navbar-expand-lg navbar-light mr-auto" id="main-nav">
    <div class="container-fluid">
        <a class="navbar-brand" href="">
            <img src="tmlr_logo.jpeg" height="40px">
            Transactions on Machine Learning Research
        </a>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav"
        aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
    </button>
    <div class="collapse navbar-collapse text-right flex-grow-1" id="navbarNav">
        <ul class="navbar-nav ml-auto">

            <!--
            <li class="nav-item dropdown">
                <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button"
                data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                Main Conference
                </a>
                <div class="dropdown-menu" aria-labelledby="navbarDropdown">
                    <div class="dropdown-divider"></div>
                    <a class="dropdown-item" href="/virtual/2023/events/oral">Orals</a>
                    <div class="dropdown-divider"></div>
                    <a class="dropdown-item" href="/virtual/2023/events/spotlight">Spotlights</a>
                    <div class="dropdown-divider"></div>
                    <a class="dropdown-item" href="/virtual/2023/papers.html">Papers</a>
                </div>
            </li>
            -->

            <!--
            <li class="nav-item">
                <a class="nav-link" href="../index_file.html">All Papers</a>
            </li>
            -->

            <li class="nav-item">
                <a class="nav-link" href="../index_file.html">Papers with Videos</a>
            </li>

            <!--
            <li class="nav-item">
                <a class="nav-link" href="../features_papers.html">Featured Papers</a>
            </li>
            -->

    <!--
    <li class="nav-item ">
        <a class="nav-link" href="/virtual/2023/search"><i class="fas fa-search"></i> &nbsp;</a>
    </li>
    -->
        </ul>
    </div>
</div>
</nav>



    <div class="container">
        <br />
        <div class="row">
            <div class="col-md-12"></div>
            <div class="title-centered" style="text-align:center">TMLR Papers with Videos</div>
        </div>
    </div>

        <div class="row">
            <div class="col-sm-12">
                <div style="max-width: 1500px; margin:auto; border">
                    <div class="grid-displaycards">
                        <div class="displaycards touchup-date" id="event-mySiFHCeAl">
    <div style="width:80%;margin:auto;">
        <a class="small-title" href="paper_pages/mySiFHCeAl.html">Spectral Regularization Allows Data-frugal Learning over Combinatorial Spaces</a>
    </div>
    <div class="type_display_name_minus_type"></div>
    <div class="author-str">Amirali Aghazadeh &middot; Nived Rajaraman &middot; Tony Tu &middot; Kannan Ramchandran</div>
    <div class="author-str higher"></div>
    <div class="text-muted touchup-date-div" id="touchup-date-event-mySiFHCeAl"></div>

    <a href="paper_pages/mySiFHCeAl.html">
        <img src="http://img.youtube.com/vi/ER12pwvxTSU/0.jpg" class="social-img-thumb rounded" alt="thumbnail"/>
    </a>

    <div class="abstract-section">
        <div>
            <a id="abstract-link-mySiFHCeAl" class="abstract-link" data-toggle="collapse"
               href="#collapse-event-abstract-mySiFHCeAl" role="button"
               aria-expanded="false" aria-controls="collapse-event-abstract-mySiFHCeAl">
                Abstract <i id="caret-mySiFHCeAl" class="fas fa-caret-right"></i>
            </a>
        </div>
    </div>

    <div class="collapse" id="collapse-event-abstract-mySiFHCeAl">
        <div class="abstract-display">
            <p>Data-driven machine learning models are being increasingly employed in several important inference problems in biology, chemistry, and physics, which require learning over combinatorial spaces. Recent empirical evidence (see, e.g., ~\cite{tseng2020fourier,aghazadeh2021epistatic,ha2021adaptive}) suggests that regularizing the spectral representation of such models improves their generalization power when labeled data is scarce. However, despite these empirical studies, the theoretical underpinning of when and how spectral regularization enables improved generalization is poorly understood. In this paper, we focus on learning pseudo-Boolean functions and demonstrate that regularizing the empirical mean squared error by the $L_1$ norm of the spectral transform of the learned function reshapes the loss landscape and allows for data-frugal learning under a restricted secant condition on the learner's empirical error measured against the ground truth function. Under a weaker quadratic growth condition, we show that stationary points, which also approximately interpolate the training data points achieve statistically optimal generalization performance. Complementing our theory, we empirically demonstrate that running gradient descent on the regularized loss results in a better generalization performance compared to baseline algorithms in several data-scarce real-world problems.</p>
        </div>
    </div>
</div>

<div class="displaycards touchup-date" id="event-y4CGF1A8VG">
    <div style="width:80%;margin:auto;">
        <a class="small-title" href="paper_pages/y4CGF1A8VG.html">Machine Explanations and Human Understanding</a>
    </div>
    <div class="type_display_name_minus_type"></div>
    <div class="author-str">Chacha Chen &middot; Shi Feng &middot; Amit Sharma &middot; Chenhao Tan</div>
    <div class="author-str higher"></div>
    <div class="text-muted touchup-date-div" id="touchup-date-event-y4CGF1A8VG"></div>

    <a href="paper_pages/y4CGF1A8VG.html">
        <img src="http://img.youtube.com/vi/-hNKPaX7L4o/0.jpg" class="social-img-thumb rounded" alt="thumbnail"/>
    </a>

    <div class="abstract-section">
        <div>
            <a id="abstract-link-y4CGF1A8VG" class="abstract-link" data-toggle="collapse"
               href="#collapse-event-abstract-y4CGF1A8VG" role="button"
               aria-expanded="false" aria-controls="collapse-event-abstract-y4CGF1A8VG">
                Abstract <i id="caret-y4CGF1A8VG" class="fas fa-caret-right"></i>
            </a>
        </div>
    </div>

    <div class="collapse" id="collapse-event-abstract-y4CGF1A8VG">
        <div class="abstract-display">
            <p>Explanations are hypothesized to improve human understanding of machine learning models and achieve a variety of desirable outcomes, ranging from model debugging to enhancing human decision making. However, empirical studies have found mixed and even negative results. An open question, therefore, is under what conditions explanations can improve human understanding and in what way. To address this question, we first identify three core concepts that cover most existing quantitative measures of understanding: task decision boundary, model decision boundary, and model error. Using adapted causal diagrams, we provide a formal characterization of the relationship between these concepts and human approximations (i.e., understanding) of them. The relationship varies by the level of human intuition in different task types, such as emulation and discovery, which are often ignored when building or evaluating explanation methods. Our key result is that human intuitions are necessary for generating and evaluating machine explanations in human-AI decision making: without assumptions about human intuitions, explanations may improve human understanding of model decision boundary, but cannot improve human understanding of task decision boundary or model error. To validate our theoretical claims, we conduct human subject studies to show the importance of human intuitions. Together with our theoretical contributions, we provide a new paradigm for designing behavioral studies towards a rigorous view of the role of machine explanations across different tasks of human-AI decision making.</p>
        </div>
    </div>
</div>

<div class="displaycards touchup-date" id="event-9aXKUJEKwV">
    <div style="width:80%;margin:auto;">
        <a class="small-title" href="paper_pages/9aXKUJEKwV.html">Learning to Look by Self-Prediction</a>
    </div>
    <div class="type_display_name_minus_type"></div>
    <div class="author-str">Matthew Koichi Grimes &middot; Joseph Varughese Modayil &middot; Piotr W Mirowski &middot; Dushyant Rao &middot; Raia Hadsell</div>
    <div class="author-str higher"></div>
    <div class="text-muted touchup-date-div" id="touchup-date-event-9aXKUJEKwV"></div>

    <a href="paper_pages/9aXKUJEKwV.html">
        <img src="https://drive.google.com/thumbnail?id=1iOFC1IWk0Jx-poBZW0lh5FcTmT55BdDw" class="social-img-thumb rounded" alt="thumbnail"/>
    </a>

    <div class="abstract-section">
        <div>
            <a id="abstract-link-9aXKUJEKwV" class="abstract-link" data-toggle="collapse"
               href="#collapse-event-abstract-9aXKUJEKwV" role="button"
               aria-expanded="false" aria-controls="collapse-event-abstract-9aXKUJEKwV">
                Abstract <i id="caret-9aXKUJEKwV" class="fas fa-caret-right"></i>
            </a>
        </div>
    </div>

    <div class="collapse" id="collapse-event-abstract-9aXKUJEKwV">
        <div class="abstract-display">
            <p>We present a method for learning active vision skills, to move the camera to observe a robot's sensors from informative points of view, without external rewards or labels. We do this by jointly training a visual predictor network, which predicts future returns of the sensors using pixels, and a camera control agent, which we reward using the negative error of the predictor. The agent thus moves the camera to points of view that are most predictive for a chosen sensor, which we select using a conditioning input to the agent. We observe that despite this noisy learned reward function, the learned policies a exhibit competence by reliably framing the sensor in a specific location in the view, an emergent location which we call a behavioral fovea. We find that replacing the conventional camera with a foveal camera further increases the policies' precision.</p>
        </div>
    </div>
</div>

<div class="displaycards touchup-date" id="event-Oq5XKRVYpQ">
    <div style="width:80%;margin:auto;">
        <a class="small-title" href="paper_pages/Oq5XKRVYpQ.html">Graph-based Multi-ODE Neural Networks for Spatio-Temporal Traffic Forecasting</a>
    </div>
    <div class="type_display_name_minus_type"></div>
    <div class="author-str">Zibo Liu &middot; Parshin Shojaee &middot; Chandan K. Reddy</div>
    <div class="author-str higher"></div>
    <div class="text-muted touchup-date-div" id="touchup-date-event-Oq5XKRVYpQ"></div>

    <a href="paper_pages/Oq5XKRVYpQ.html">
        <img src="http://img.youtube.com/vi/00SItN8IP-M/0.jpg" class="social-img-thumb rounded" alt="thumbnail"/>
    </a>

    <div class="abstract-section">
        <div>
            <a id="abstract-link-Oq5XKRVYpQ" class="abstract-link" data-toggle="collapse"
               href="#collapse-event-abstract-Oq5XKRVYpQ" role="button"
               aria-expanded="false" aria-controls="collapse-event-abstract-Oq5XKRVYpQ">
                Abstract <i id="caret-Oq5XKRVYpQ" class="fas fa-caret-right"></i>
            </a>
        </div>
    </div>

    <div class="collapse" id="collapse-event-abstract-Oq5XKRVYpQ">
        <div class="abstract-display">
            <p>There is a recent surge in the development of spatio-temporal forecasting models in the transportation domain. Long-range traffic forecasting, however,  remains a challenging task due to the intricate and extensive spatio-temporal correlations observed in traffic networks. 
Current works primarily rely on road networks with graph structures and learn representations using graph neural networks (GNNs), but this approach suffers from over-smoothing problem in deep architectures. To tackle this problem, recent methods introduced the combination of GNNs with residual connections or neural ordinary differential equations (ODE). However, current graph ODE models face two key limitations in feature extraction: (1) they lean towards global temporal patterns, overlooking local patterns that are important for unexpected events; and (2) they lack dynamic semantic edges in their architectural design. In this paper, we propose a novel architecture called Graph-based Multi-ODE Neural Networks (GRAM-ODE) which is designed with multiple connective ODE-GNN modules to learn better representations by capturing different views of complex local and global dynamic spatio-temporal dependencies. We also add some techniques like shared weights and divergence constraints into the intermediate layers of distinct ODE-GNN modules to further improve their communication towards the forecasting task. Our extensive set of experiments conducted on six real-world datasets demonstrate the superior performance of GRAM-ODE compared with state-of-the-art baselines as well as the contribution of different components to the overall performance. </p>
        </div>
    </div>
</div>

<div class="displaycards touchup-date" id="event-YwNrPLjHSL">
    <div style="width:80%;margin:auto;">
        <a class="small-title" href="paper_pages/YwNrPLjHSL.html">Do Vision-Language Pretrained Models Learn Composable Primitive Concepts?</a>
    </div>
    <div class="type_display_name_minus_type"></div>
    <div class="author-str">Tian Yun &middot; Usha Bhalla &middot; Ellie Pavlick &middot; Chen Sun</div>
    <div class="author-str higher"></div>
    <div class="text-muted touchup-date-div" id="touchup-date-event-YwNrPLjHSL"></div>

    <a href="paper_pages/YwNrPLjHSL.html">
        <img src="http://img.youtube.com/vi/qerql58NeeE/0.jpg" class="social-img-thumb rounded" alt="thumbnail"/>
    </a>

    <div class="abstract-section">
        <div>
            <a id="abstract-link-YwNrPLjHSL" class="abstract-link" data-toggle="collapse"
               href="#collapse-event-abstract-YwNrPLjHSL" role="button"
               aria-expanded="false" aria-controls="collapse-event-abstract-YwNrPLjHSL">
                Abstract <i id="caret-YwNrPLjHSL" class="fas fa-caret-right"></i>
            </a>
        </div>
    </div>

    <div class="collapse" id="collapse-event-abstract-YwNrPLjHSL">
        <div class="abstract-display">
            <p>Vision-language (VL) pretrained models have achieved impressive performance on multimodal reasoning and zero-shot recognition tasks. Many of these VL models are pretrained on unlabeled image and caption pairs from the internet. In this paper, we study whether representations of primitive concepts–such as colors, shapes, or the attributes of object parts–emerge automatically within these pretrained VL models. We propose a two-step framework, Compositional Concept Mapping (CompMap), to investigate this. CompMap
first asks a VL model to generate concept activations with text prompts from a predefined list of primitive concepts, and then learns to construct an explicit composition model that maps the primitive concept activations (e.g. the likelihood of black tail or red wing) to com-
posite concepts (e.g. a red-winged blackbird). We demonstrate that a composition model can be designed as a set operation, and show that a composition model is straightforward for machines to learn from ground truth primitive concepts (as a linear classifier). We thus
hypothesize that if primitive concepts indeed emerge in a VL pretrained model, its primitive concept activations can be used to learn a composition model similar to the one designed by experts. We propose a quantitative metric to measure the degree of similarity, and refer to the metric as the interpretability of the VL models’ learned primitive concept representations. We also measure the classification accuracy when using the primitive concept activations and the learned composition model to predict the composite concepts, and refer to it as the usefulness metric. Our study reveals that state-of-the-art VL pretrained models learn primitive concepts that are highly useful for fine-grained visual recognition on the CUB dataset, and compositional generalization tasks on the MIT-States dataset. However,
we observe that the learned composition models have low interpretability in our qualitative analyses. Our results reveal the limitations of existing VL models, and the necessity of pretraining objectives that encourage the acquisition of primitive concepts.</p>
        </div>
    </div>
</div>

<div class="displaycards touchup-date" id="event-fvyh6mDWFr">
    <div style="width:80%;margin:auto;">
        <a class="small-title" href="paper_pages/fvyh6mDWFr.html">Understanding Noise-Augmented Training for Randomized Smoothing</a>
    </div>
    <div class="type_display_name_minus_type"></div>
    <div class="author-str">Ambar Pal &middot; Jeremias Sulam</div>
    <div class="author-str higher"></div>
    <div class="text-muted touchup-date-div" id="touchup-date-event-fvyh6mDWFr"></div>

    <a href="paper_pages/fvyh6mDWFr.html">
        <img src="http://img.youtube.com/vi/LzIoT5pX7pQ/0.jpg" class="social-img-thumb rounded" alt="thumbnail"/>
    </a>

    <div class="abstract-section">
        <div>
            <a id="abstract-link-fvyh6mDWFr" class="abstract-link" data-toggle="collapse"
               href="#collapse-event-abstract-fvyh6mDWFr" role="button"
               aria-expanded="false" aria-controls="collapse-event-abstract-fvyh6mDWFr">
                Abstract <i id="caret-fvyh6mDWFr" class="fas fa-caret-right"></i>
            </a>
        </div>
    </div>

    <div class="collapse" id="collapse-event-abstract-fvyh6mDWFr">
        <div class="abstract-display">
            <p>Randomized smoothing is a technique for providing provable robustness guarantees against adversarial attacks while making minimal assumptions about a classifier. This method relies on taking a majority vote of any base classifier over multiple noise-perturbed inputs to obtain a smoothed classifier, and it remains the tool of choice to certify deep and complex neural network models. Nonetheless, non-trivial performance of such smoothed classifier crucially depends on the base model being trained on noise-augmented data, i.e., on a smoothed input distribution. While widely adopted in practice, it is still unclear how this noisy training of the base classifier precisely affects the risk of the robust smoothed classifier, leading to heuristics and tricks that are poorly understood. In this work we analyze these trade-offs theoretically in a binary classification setting, proving that these common observations are not universal. We show that, without making stronger distributional assumptions, no benefit can be expected from predictors trained with noise-augmentation, and we further characterize distributions where such benefit is obtained. Our analysis has direct implications to the practical deployment of randomized smoothing, and we illustrate some of these via experiments on CIFAR-10 and MNIST, as well as on synthetic datasets.</p>
        </div>
    </div>
</div>

<div class="displaycards touchup-date" id="event-SM1BkjGePI">
    <div style="width:80%;margin:auto;">
        <a class="small-title" href="paper_pages/SM1BkjGePI.html">Bridging performance gap between minimal and maximal SVM models</a>
    </div>
    <div class="type_display_name_minus_type"></div>
    <div class="author-str">Ondrej Such &middot; René Fabricius</div>
    <div class="author-str higher"></div>
    <div class="text-muted touchup-date-div" id="touchup-date-event-SM1BkjGePI"></div>

    <a href="paper_pages/SM1BkjGePI.html">
        <img src="http://img.youtube.com/vi/jWAfN4deoC8/0.jpg" class="social-img-thumb rounded" alt="thumbnail"/>
    </a>

    <div class="abstract-section">
        <div>
            <a id="abstract-link-SM1BkjGePI" class="abstract-link" data-toggle="collapse"
               href="#collapse-event-abstract-SM1BkjGePI" role="button"
               aria-expanded="false" aria-controls="collapse-event-abstract-SM1BkjGePI">
                Abstract <i id="caret-SM1BkjGePI" class="fas fa-caret-right"></i>
            </a>
        </div>
    </div>

    <div class="collapse" id="collapse-event-abstract-SM1BkjGePI">
        <div class="abstract-display">
            <p>Multi-class support vector machine (SVM) models are typically built using all possible pairs of binary SVM in a one-against-one fashion. This requires too much computation for datasets with hundreds or thousands of classes,  which motivates the search for multi-class models that do not use all pairwise SVM.  Our models correspond to the choice of the model graph, whose vertices correspond to classes and edges represent which pairwise SVMs are trained. We conduct experiments to uncover metrical and topological properties that impact the accuracy of a multi-class SVM model. Based on their results we propose a way to construct intermediate multi-class SVM models. The key insight is that for model graphs of diameter two, we can estimate missing pairwise probabilities from the known ones thus transforming the computation of posteriors to the usual complete (maximal) case. Our proposed algorithm allows one to reduce computational effort by 50-80% while keeping accuracy near, or even above that of a softmax classifier. In our work we use convolutional data sets, which have multiple advantages for benchmarking multi-class SVM models.</p>
        </div>
    </div>
</div>

<div class="displaycards touchup-date" id="event-5aYGXxByI6">
    <div style="width:80%;margin:auto;">
        <a class="small-title" href="paper_pages/5aYGXxByI6.html">MASIF: Meta-learned Algorithm Selection using Implicit Fidelity Information</a>
    </div>
    <div class="type_display_name_minus_type"></div>
    <div class="author-str">Tim Ruhkopf &middot; Aditya Mohan &middot; Difan Deng &middot; Alexander Tornede &middot; Frank Hutter &middot; Marius Lindauer</div>
    <div class="author-str higher"></div>
    <div class="text-muted touchup-date-div" id="touchup-date-event-5aYGXxByI6"></div>

    <a href="paper_pages/5aYGXxByI6.html">
        <img src="http://img.youtube.com/vi/4qXRyRjJPIY/0.jpg" class="social-img-thumb rounded" alt="thumbnail"/>
    </a>

    <div class="abstract-section">
        <div>
            <a id="abstract-link-5aYGXxByI6" class="abstract-link" data-toggle="collapse"
               href="#collapse-event-abstract-5aYGXxByI6" role="button"
               aria-expanded="false" aria-controls="collapse-event-abstract-5aYGXxByI6">
                Abstract <i id="caret-5aYGXxByI6" class="fas fa-caret-right"></i>
            </a>
        </div>
    </div>

    <div class="collapse" id="collapse-event-abstract-5aYGXxByI6">
        <div class="abstract-display">
            <p>Selecting a well-performing algorithm for a given task or dataset can be time-consuming and
tedious, but is crucial for the successful day-to-day business of developing new AI & ML
applications. Algorithm Selection (AS) mitigates this through a meta-model leveraging
meta-information about previous tasks. However, most of the available AS methods are
error-prone because they characterize a task by either cheap-to-compute properties of the
dataset or evaluations of cheap proxy algorithms, called landmarks. In this work, we extend
the classical AS data setup to include multi-fidelity information and empirically demonstrate
how meta-learning on algorithms’ learning behaviour allows us to exploit cheap test-time
evidence effectively and combat myopia significantly. We further postulate a budget-regret
trade-off w.r.t. the selection process. Our new selector MASIF is able to jointly interpret
online evidence on a task in form of varying-length learning curves without any parametric
assumption by leveraging a transformer-based encoder. This opens up new possibilities for
guided rapid prototyping in data science on cheaply observed partial learning curves.</p>
        </div>
    </div>
</div>

<div class="displaycards touchup-date" id="event-oq3tx5kinu">
    <div style="width:80%;margin:auto;">
        <a class="small-title" href="paper_pages/oq3tx5kinu.html">Active Learning of Ordinal Embeddings: A User Study on Football Data</a>
    </div>
    <div class="type_display_name_minus_type"></div>
    <div class="author-str">Christoffer Löffler &middot; Kion Fallah &middot; Stefano Fenu &middot; Dario Zanca &middot; Bjoern Eskofier &middot; Christopher John Rozell &middot; Christopher Mutschler</div>
    <div class="author-str higher"></div>
    <div class="text-muted touchup-date-div" id="touchup-date-event-oq3tx5kinu"></div>

    <a href="paper_pages/oq3tx5kinu.html">
        <img src="http://img.youtube.com/vi/xqOJAtjxjKE/0.jpg" class="social-img-thumb rounded" alt="thumbnail"/>
    </a>

    <div class="abstract-section">
        <div>
            <a id="abstract-link-oq3tx5kinu" class="abstract-link" data-toggle="collapse"
               href="#collapse-event-abstract-oq3tx5kinu" role="button"
               aria-expanded="false" aria-controls="collapse-event-abstract-oq3tx5kinu">
                Abstract <i id="caret-oq3tx5kinu" class="fas fa-caret-right"></i>
            </a>
        </div>
    </div>

    <div class="collapse" id="collapse-event-abstract-oq3tx5kinu">
        <div class="abstract-display">
            <p>Humans innately measure distance between instances in an unlabeled dataset using an unknown similarity function. Distance metrics can only serve as proxy for similarity in information retrieval of similar instances. Learning a good similarity function from human annotations improves the quality of retrievals. This work uses deep metric learning to learn these user-defined similarity functions from few annotations for a large football trajectory dataset.
We adapt an entropy-based active learning method with recent work from triplet mining to collect easy-to-answer but still informative annotations from human participants and use them to train a deep convolutional network that generalizes to unseen samples. 
Our user study shows that our approach improves the quality of the information retrieval compared to a previous deep metric learning approach that relies on a Siamese network. Specifically, we shed light on the strengths and weaknesses of passive sampling heuristics and active learners alike by analyzing the participants' response efficacy. To this end, we collect accuracy, algorithmic time complexity, the participants' fatigue and time-to-response, qualitative self-assessment and statements, as well as the effects of mixed-expertise annotators and their consistency on model performance and transfer-learning.
</p>
        </div>
    </div>
</div>

<div class="displaycards touchup-date" id="event-a0T3nOP9sB">
    <div style="width:80%;margin:auto;">
        <a class="small-title" href="paper_pages/a0T3nOP9sB.html">Adaptive patch foraging in deep reinforcement learning agents</a>
    </div>
    <div class="type_display_name_minus_type"></div>
    <div class="author-str">Nathan Wispinski &middot; Andrew Butcher &middot; Kory Wallace Mathewson &middot; Craig S Chapman &middot; Matthew Botvinick &middot; Patrick M. Pilarski</div>
    <div class="author-str higher"></div>
    <div class="text-muted touchup-date-div" id="touchup-date-event-a0T3nOP9sB"></div>

    <a href="paper_pages/a0T3nOP9sB.html">
        <img src="http://img.youtube.com/vi/5PfxPj5Jzwo/0.jpg" class="social-img-thumb rounded" alt="thumbnail"/>
    </a>

    <div class="abstract-section">
        <div>
            <a id="abstract-link-a0T3nOP9sB" class="abstract-link" data-toggle="collapse"
               href="#collapse-event-abstract-a0T3nOP9sB" role="button"
               aria-expanded="false" aria-controls="collapse-event-abstract-a0T3nOP9sB">
                Abstract <i id="caret-a0T3nOP9sB" class="fas fa-caret-right"></i>
            </a>
        </div>
    </div>

    <div class="collapse" id="collapse-event-abstract-a0T3nOP9sB">
        <div class="abstract-display">
            <p>Patch foraging is one of the most heavily studied behavioral optimization challenges in biology. However, despite its importance to biological intelligence, this behavioral optimization problem is understudied in artificial intelligence research. Patch foraging is especially amenable to study given that it has a known optimal solution, which may be difficult to discover given current techniques in deep reinforcement learning. Here, we investigate deep reinforcement learning agents in an ecological patch foraging task. For the first time, we show that machine learning agents can learn to patch forage adaptively in patterns similar to biological foragers, and approach optimal patch foraging behavior when accounting for temporal discounting. Finally, we show emergent internal dynamics in these agents that resemble single-cell recordings from foraging non-human primates, which complements experimental and theoretical work on the neural mechanisms of biological foraging. This work suggests that agents interacting in complex environments with ecologically valid pressures arrive at common solutions, suggesting the emergence of foundational computations behind adaptive, intelligent behavior in both biological and artificial agents.</p>
        </div>
    </div>
</div>

<div class="displaycards touchup-date" id="event-onufdyHvqN">
    <div style="width:80%;margin:auto;">
        <a class="small-title" href="paper_pages/onufdyHvqN.html">Private Multi-Task Learning: Formulation and Applications to Federated Learning</a>
    </div>
    <div class="type_display_name_minus_type"></div>
    <div class="author-str">Shengyuan Hu &middot; Steven Wu &middot; Virginia Smith</div>
    <div class="author-str higher"></div>
    <div class="text-muted touchup-date-div" id="touchup-date-event-onufdyHvqN"></div>

    <a href="paper_pages/onufdyHvqN.html">
        <img src="https://drive.google.com/thumbnail?id=1J35lwtPfLtEF3HpomNKYc3CzEW6nhgu6" class="social-img-thumb rounded" alt="thumbnail"/>
    </a>

    <div class="abstract-section">
        <div>
            <a id="abstract-link-onufdyHvqN" class="abstract-link" data-toggle="collapse"
               href="#collapse-event-abstract-onufdyHvqN" role="button"
               aria-expanded="false" aria-controls="collapse-event-abstract-onufdyHvqN">
                Abstract <i id="caret-onufdyHvqN" class="fas fa-caret-right"></i>
            </a>
        </div>
    </div>

    <div class="collapse" id="collapse-event-abstract-onufdyHvqN">
        <div class="abstract-display">
            <p>Many problems in machine learning rely on multi-task learning (MTL), in which the goal is to solve multiple related machine learning tasks simultaneously. MTL is particularly relevant for privacy-sensitive applications in areas such as healthcare, finance, and IoT computing,
where sensitive data from multiple, varied sources are shared for the purpose of learning. In this work, we formalize notions of client-level privacy for MTL via billboard privacy (BP), a relaxation of differential privacy for mechanism design and distributed optimization. We then propose an algorithm for mean-regularized MTL, an objective commonly used for applications in personalized federated learning, subject to BP. We analyze our objective and solver, providing certifiable guarantees on both privacy and utility. Empirically, we find that our method provides improved privacy/utility trade-offs relative to global baselines across common federated learning benchmarks.</p>
        </div>
    </div>
</div>

<div class="displaycards touchup-date" id="event-4eL6z9ziw7">
    <div style="width:80%;margin:auto;">
        <a class="small-title" href="paper_pages/4eL6z9ziw7.html">NovelCraft: A Dataset for Novelty Detection and Discovery in Open Worlds</a>
    </div>
    <div class="type_display_name_minus_type"></div>
    <div class="author-str">Patrick Feeney &middot; Sarah Schneider &middot; Panagiotis Lymperopoulos &middot; Liping Liu &middot; Matthias Scheutz &middot; Michael C Hughes</div>
    <div class="author-str higher"></div>
    <div class="text-muted touchup-date-div" id="touchup-date-event-4eL6z9ziw7"></div>

    <a href="paper_pages/4eL6z9ziw7.html">
        <img src="http://img.youtube.com/vi/d0AAH_eMPoQ/0.jpg" class="social-img-thumb rounded" alt="thumbnail"/>
    </a>

    <div class="abstract-section">
        <div>
            <a id="abstract-link-4eL6z9ziw7" class="abstract-link" data-toggle="collapse"
               href="#collapse-event-abstract-4eL6z9ziw7" role="button"
               aria-expanded="false" aria-controls="collapse-event-abstract-4eL6z9ziw7">
                Abstract <i id="caret-4eL6z9ziw7" class="fas fa-caret-right"></i>
            </a>
        </div>
    </div>

    <div class="collapse" id="collapse-event-abstract-4eL6z9ziw7">
        <div class="abstract-display">
            <p>In order for artificial agents to successfully perform tasks in changing environments, they must be able to both detect and adapt to novelty. However, visual novelty detection research often only evaluates on repurposed datasets such as CIFAR-10 originally intended for object classification, where images focus on one distinct, well-centered object. New benchmarks are needed to represent the challenges of navigating the complex scenes of an open world. Our new NovelCraft dataset contains multimodal episodic data of the images and symbolic world-states seen by an agent completing a pogo stick assembly task within a modified Minecraft environment. In some episodes, we insert novel objects of varying size within the complex 3D scene that may impact gameplay. Our visual novelty detection benchmark finds that methods that rank best on popular area-under-the-curve metrics may be outperformed by simpler alternatives when controlling false positives matters most. Further multimodal novelty detection experiments suggest that methods that fuse both visual and symbolic information can improve time until detection as well as overall discrimination. Finally, our evaluation of recent generalized category discovery methods suggests that adapting to new imbalanced categories in complex scenes remains an exciting open problem.</p>
        </div>
    </div>
</div>

<div class="displaycards touchup-date" id="event-jdGMBgYvfX">
    <div style="width:80%;margin:auto;">
        <a class="small-title" href="paper_pages/jdGMBgYvfX.html">UncertaINR: Uncertainty Quantification of End-to-End Implicit Neural Representations for Computed Tomography</a>
    </div>
    <div class="type_display_name_minus_type"></div>
    <div class="author-str">Francisca Vasconcelos &middot; Bobby He &middot; Nalini M Singh &middot; Yee Whye Teh</div>
    <div class="author-str higher"></div>
    <div class="text-muted touchup-date-div" id="touchup-date-event-jdGMBgYvfX"></div>

    <a href="paper_pages/jdGMBgYvfX.html">
        <img src="http://img.youtube.com/vi/cD7Wx4F_EjQ/0.jpg" class="social-img-thumb rounded" alt="thumbnail"/>
    </a>

    <div class="abstract-section">
        <div>
            <a id="abstract-link-jdGMBgYvfX" class="abstract-link" data-toggle="collapse"
               href="#collapse-event-abstract-jdGMBgYvfX" role="button"
               aria-expanded="false" aria-controls="collapse-event-abstract-jdGMBgYvfX">
                Abstract <i id="caret-jdGMBgYvfX" class="fas fa-caret-right"></i>
            </a>
        </div>
    </div>

    <div class="collapse" id="collapse-event-abstract-jdGMBgYvfX">
        <div class="abstract-display">
            <p>Implicit neural representations (INRs) have achieved impressive results for scene reconstruction and computer graphics, where their performance has primarily been assessed on reconstruction accuracy. As INRs make their way into other domains, where model predictions inform high-stakes decision-making, uncertainty quantification of INR inference is becoming critical. To that end, we study a Bayesian reformulation of INRs, UncertaINR, in the context of computed tomography, and evaluate several Bayesian deep learning implementations in terms of accuracy and calibration.  We find that they achieve well-calibrated uncertainty, while retaining accuracy competitive with other classical, INR-based, and CNN-based reconstruction techniques. Contrary to common intuition in the Bayesian deep learning literature, we find that INRs obtain the best calibration with computationally efficient Monte Carlo dropout, outperforming Hamiltonian Monte Carlo and deep ensembles. Moreover, in contrast to the best-performing prior approaches, UncertaINR does not require a large training dataset, but only a handful of validation images.</p>
        </div>
    </div>
</div>

<div class="displaycards touchup-date" id="event-MRLHN4MSmA">
    <div style="width:80%;margin:auto;">
        <a class="small-title" href="paper_pages/MRLHN4MSmA.html">A Modulation Layer to Increase Neural Network Robustness Against Data Quality Issues</a>
    </div>
    <div class="type_display_name_minus_type"></div>
    <div class="author-str">Mohamed Abdelhack &middot; Jiaming Zhang &middot; Sandhya Tripathi &middot; Bradley A Fritz &middot; Daniel Felsky &middot; Michael Avidan &middot; Yixin Chen &middot; Christopher Ryan King</div>
    <div class="author-str higher"></div>
    <div class="text-muted touchup-date-div" id="touchup-date-event-MRLHN4MSmA"></div>

    <a href="paper_pages/MRLHN4MSmA.html">
        <img src="http://img.youtube.com/vi/SI-5cuPJV9U/0.jpg" class="social-img-thumb rounded" alt="thumbnail"/>
    </a>

    <div class="abstract-section">
        <div>
            <a id="abstract-link-MRLHN4MSmA" class="abstract-link" data-toggle="collapse"
               href="#collapse-event-abstract-MRLHN4MSmA" role="button"
               aria-expanded="false" aria-controls="collapse-event-abstract-MRLHN4MSmA">
                Abstract <i id="caret-MRLHN4MSmA" class="fas fa-caret-right"></i>
            </a>
        </div>
    </div>

    <div class="collapse" id="collapse-event-abstract-MRLHN4MSmA">
        <div class="abstract-display">
            <p>Data missingness and quality are common problems in machine learning, especially for high-stakes applications such as healthcare. Developers often train machine learning models on carefully curated datasets using only high-quality data; however, this reduces the utility of such models in production environments. We propose a novel neural network modification to mitigate the impacts of low-quality and missing data which involves replacing the fixed weights of a fully-connected layer with a function of additional input. This is inspired by neuromodulation in biological neural networks where the cortex can up- and down-regulate inputs based on their reliability and the presence of other data. In testing, with reliability scores as a modulating signal, models with modulating layers were found to be more robust against data quality degradation, including additional missingness. These models are superior to imputation as they save on training time by entirely skipping the imputation process and further allow the introduction of other data quality measures that imputation cannot handle. Our results suggest that explicitly accounting for reduced information quality with a modulating fully connected layer can enable the deployment of artificial intelligence systems in real-time applications.</p>
        </div>
    </div>
</div>

<div class="displaycards touchup-date" id="event-gR9UVgH8PZ">
    <div style="width:80%;margin:auto;">
        <a class="small-title" href="paper_pages/gR9UVgH8PZ.html">Neural Shape Compiler: A Unified Framework for Transforming between Text, Point Cloud, and Program</a>
    </div>
    <div class="type_display_name_minus_type"></div>
    <div class="author-str">Tiange Luo &middot; Honglak Lee &middot; Justin Johnson</div>
    <div class="author-str higher"></div>
    <div class="text-muted touchup-date-div" id="touchup-date-event-gR9UVgH8PZ"></div>

    <a href="paper_pages/gR9UVgH8PZ.html">
        <img src="tmlr_logo.jpeg" class="social-img-thumb rounded" alt="thumbnail"/>
    </a>

    <div class="abstract-section">
        <div>
            <a id="abstract-link-gR9UVgH8PZ" class="abstract-link" data-toggle="collapse"
               href="#collapse-event-abstract-gR9UVgH8PZ" role="button"
               aria-expanded="false" aria-controls="collapse-event-abstract-gR9UVgH8PZ">
                Abstract <i id="caret-gR9UVgH8PZ" class="fas fa-caret-right"></i>
            </a>
        </div>
    </div>

    <div class="collapse" id="collapse-event-abstract-gR9UVgH8PZ">
        <div class="abstract-display">
            <p>3D shapes have complementary abstractions from low-level geometry to part-based hierarchies to languages, which convey different levels of information. This paper presents a unified framework to translate between pairs of shape abstractions: $\textit{Text}$ $\Longleftrightarrow$ $\textit{Point Cloud}$ $\Longleftrightarrow$ $\textit{Program}$. We propose $\textbf{\textit{Neural Shape Compiler}}$ to model the abstraction transformation as a conditional generation process. It converts 3D shapes of three abstract types into unified discrete shape code, transforms each shape code into code of other abstract types through the proposed $\textit{ShapeCode Transformer}$, and decodes them to output the target shape abstraction. Point Cloud code is obtained in a class-agnostic way by the proposed $\textit{Point}$VQVAE. On Text2Shape, ShapeGlot, ABO, Genre, and Program Synthetic datasets, Neural Shape Compiler shows strengths in $\textit{Text}$ $\Longrightarrow$ $\textit{Point Cloud}$, $\textit{Point Cloud}$ $\Longrightarrow$ $\textit{Text}$, $\textit{Point Cloud}$ $\Longrightarrow$ $\textit{Program}$, and Point Cloud Completion tasks. Additionally, Neural Shape Compiler benefits from jointly training on all heterogeneous data and tasks.</p>
        </div>
    </div>
</div>

<div class="displaycards touchup-date" id="event-v5jwDLqfQo">
    <div style="width:80%;margin:auto;">
        <a class="small-title" href="paper_pages/v5jwDLqfQo.html">Extended Agriculture-Vision: An Extension of a Large Aerial Image Dataset for Agricultural Pattern Analysis</a>
    </div>
    <div class="type_display_name_minus_type"></div>
    <div class="author-str">Jing Wu &middot; David Pichler &middot; Daniel Marley &middot; Naira Hovakimyan &middot; David A Wilson &middot; Jennifer Hobbs</div>
    <div class="author-str higher"></div>
    <div class="text-muted touchup-date-div" id="touchup-date-event-v5jwDLqfQo"></div>

    <a href="paper_pages/v5jwDLqfQo.html">
        <img src="http://img.youtube.com/vi/2xaKxUpY4iQ/0.jpg" class="social-img-thumb rounded" alt="thumbnail"/>
    </a>

    <div class="abstract-section">
        <div>
            <a id="abstract-link-v5jwDLqfQo" class="abstract-link" data-toggle="collapse"
               href="#collapse-event-abstract-v5jwDLqfQo" role="button"
               aria-expanded="false" aria-controls="collapse-event-abstract-v5jwDLqfQo">
                Abstract <i id="caret-v5jwDLqfQo" class="fas fa-caret-right"></i>
            </a>
        </div>
    </div>

    <div class="collapse" id="collapse-event-abstract-v5jwDLqfQo">
        <div class="abstract-display">
            <p>A key challenge for much of the machine learning work on remote sensing and earth observation data is the difficulty in acquiring large amounts of accurately labeled data. This is particularly true for semantic segmentation tasks, which are much less common in the remote sensing domain because of the incredible difficulty in collecting precise, accurate, pixel-level annotations at scale. Recent efforts have addressed these challenges both through the creation of supervised datasets as well as the application of self-supervised methods. We continue these efforts on both fronts. First, we generate and release an improved version of the Agriculture-Vision dataset  (Chiu et al., 2020b) to include raw, full-field imagery for greater experimental flexibility. Second, we extend this dataset with the release of 3600 large, high-resolution (10cm/pixel), full-field, red-green-blue and near-infrared images for pre-training. Third, we incorporate the Pixel-to-Propagation Module Xie et al. (2021b) originally built on the SimCLR framework into the framework of MoCo-V2 Chen et al.(2020b). Finally, we demonstrate the usefulness of this data by benchmarking different contrastive learning approaches on both downstream classification and semantic segmentation tasks. We explore both CNN and Swin Transformer Liu et al. (2021a) architectures within different frameworks based on MoCo-V2. Together, these approaches enable us to better detect key agricultural patterns of interest across a field from aerial imagery so that farmers may be alerted to problematic areas in a timely fashion to inform their management decisions. Furthermore, the release of these datasets will support numerous avenues of research for computer vision in remote sensing for agriculture.</p>
        </div>
    </div>
</div>

<div class="displaycards touchup-date" id="event-OJtYpdiHNo">
    <div style="width:80%;margin:auto;">
        <a class="small-title" href="paper_pages/OJtYpdiHNo.html">Transframer: Arbitrary Frame Prediction with Generative Models</a>
    </div>
    <div class="type_display_name_minus_type"></div>
    <div class="author-str">Charlie Nash &middot; Joao Carreira &middot; Jacob C Walker &middot; Iain Barr &middot; Andrew Jaegle &middot; Mateusz Malinowski &middot; Peter Battaglia</div>
    <div class="author-str higher"></div>
    <div class="text-muted touchup-date-div" id="touchup-date-event-OJtYpdiHNo"></div>

    <a href="paper_pages/OJtYpdiHNo.html">
        <img src="tmlr_logo.jpeg" class="social-img-thumb rounded" alt="thumbnail"/>
    </a>

    <div class="abstract-section">
        <div>
            <a id="abstract-link-OJtYpdiHNo" class="abstract-link" data-toggle="collapse"
               href="#collapse-event-abstract-OJtYpdiHNo" role="button"
               aria-expanded="false" aria-controls="collapse-event-abstract-OJtYpdiHNo">
                Abstract <i id="caret-OJtYpdiHNo" class="fas fa-caret-right"></i>
            </a>
        </div>
    </div>

    <div class="collapse" id="collapse-event-abstract-OJtYpdiHNo">
        <div class="abstract-display">
            <p>We present a general-purpose framework for image modelling and vision tasks based on probabilistic frame prediction. Our approach unifies a broad range of tasks, from image segmentation, to novel view synthesis and video interpolation. We pair this framework with an architecture we term \modelname, which uses U-Net and Transformer components to condition on annotated context frames, and outputs sequences of sparse, compressed image features. Transframer is the state-of-the-art on a variety of video generation benchmarks, is competitive with the strongest models on few-shot view synthesis, and can generate coherent 30 second videos from a single image without any explicit geometric information. 

A single generalist Transframer simultaneously produces promising results on 8 tasks, including semantic segmentation, image classification and optical flow prediction with no task-specific architectural components, demonstrating that multi-task computer vision can be tackled using probabilistic image models.

Our approach can in principle be applied to a wide range of applications that require learning the conditional structure of annotated image-formatted data.</p>
        </div>
    </div>
</div>

<div class="displaycards touchup-date" id="event-qdDmxzGuzu">
    <div style="width:80%;margin:auto;">
        <a class="small-title" href="paper_pages/qdDmxzGuzu.html">Reusable Options through Gradient-based Meta Learning</a>
    </div>
    <div class="type_display_name_minus_type"></div>
    <div class="author-str">David Kuric &middot; Herke van Hoof</div>
    <div class="author-str higher"></div>
    <div class="text-muted touchup-date-div" id="touchup-date-event-qdDmxzGuzu"></div>

    <a href="paper_pages/qdDmxzGuzu.html">
        <img src="http://img.youtube.com/vi/Dp5a20y9ohw/0.jpg" class="social-img-thumb rounded" alt="thumbnail"/>
    </a>

    <div class="abstract-section">
        <div>
            <a id="abstract-link-qdDmxzGuzu" class="abstract-link" data-toggle="collapse"
               href="#collapse-event-abstract-qdDmxzGuzu" role="button"
               aria-expanded="false" aria-controls="collapse-event-abstract-qdDmxzGuzu">
                Abstract <i id="caret-qdDmxzGuzu" class="fas fa-caret-right"></i>
            </a>
        </div>
    </div>

    <div class="collapse" id="collapse-event-abstract-qdDmxzGuzu">
        <div class="abstract-display">
            <p>Hierarchical methods in reinforcement learning have the potential to reduce the amount of decisions that the agent needs to perform when learning new tasks. However, finding a reusable useful temporal abstractions that facilitate fast learning remains a challenging problem. Recently, several deep learning approaches were proposed to learn such temporal abstractions in the form of options in an end-to-end manner. In this work, we point out several shortcomings of these methods and discuss their potential negative consequences. Subsequently, we formulate the desiderata for reusable options and use these to frame the problem of learning options as a gradient-based meta-learning problem. This allows us to formulate an objective that explicitly incentivizes options which allow a higher-level decision maker to adjust in few steps to different tasks. Experimentally, we show that our method is able to learn transferable components which accelerate learning and performs better than existing prior methods developed for this setting. Additionally, we perform ablations to quantify the impact of using gradient-based meta-learning as well as other proposed changes.</p>
        </div>
    </div>
</div>

<div class="displaycards touchup-date" id="event-qvRWcDXBam">
    <div style="width:80%;margin:auto;">
        <a class="small-title" href="paper_pages/qvRWcDXBam.html">Containing a spread through sequential learning: to exploit or to explore?</a>
    </div>
    <div class="type_display_name_minus_type"></div>
    <div class="author-str">Xingran Chen &middot; Hesam Nikpey &middot; Jungyeol Kim &middot; Saswati Sarkar &middot; Shirin Saeedi Bidokhti</div>
    <div class="author-str higher"></div>
    <div class="text-muted touchup-date-div" id="touchup-date-event-qvRWcDXBam"></div>

    <a href="paper_pages/qvRWcDXBam.html">
        <img src="tmlr_logo.jpeg" class="social-img-thumb rounded" alt="thumbnail"/>
    </a>

    <div class="abstract-section">
        <div>
            <a id="abstract-link-qvRWcDXBam" class="abstract-link" data-toggle="collapse"
               href="#collapse-event-abstract-qvRWcDXBam" role="button"
               aria-expanded="false" aria-controls="collapse-event-abstract-qvRWcDXBam">
                Abstract <i id="caret-qvRWcDXBam" class="fas fa-caret-right"></i>
            </a>
        </div>
    </div>

    <div class="collapse" id="collapse-event-abstract-qvRWcDXBam">
        <div class="abstract-display">
            <p>The spread of an undesirable contact process, such as an infectious disease (e.g. COVID-19), is contained through testing and  isolation of infected nodes. The temporal and spatial evolution of the process (along with containment through isolation) render such detection as fundamentally different from active search detection strategies. In this work, through an active learning approach, we design testing and isolation strategies to contain the spread and minimize the cumulative infections under a given test budget. We prove that the objective can be optimized, with performance guarantees,  by greedily selecting the nodes to test.  We further design reward-based methodologies that effectively minimize an upper bound on the cumulative infections and are computationally more tractable in large networks. These policies, however, need knowledge about the nodes' infection probabilities which are dynamically changing and have to be learned by sequential testing. We develop a message-passing framework for this purpose and, building on that,  show novel tradeoffs between exploitation of knowledge through reward-based heuristics and exploration of the unknown through a carefully designed probabilistic testing. The tradeoffs are fundamentally distinct from the classical counterparts under active search or multi-armed bandit problems (MABs). We provably show the necessity of exploration in a stylized network and show through simulations that exploration can outperform exploitation in various synthetic and real-data networks depending on the parameters of the network and the spread.</p>
        </div>
    </div>
</div>

<div class="displaycards touchup-date" id="event-EYrRzKPinA">
    <div style="width:80%;margin:auto;">
        <a class="small-title" href="paper_pages/EYrRzKPinA.html">On a continuous time model of gradient descent dynamics and instability in deep learning</a>
    </div>
    <div class="type_display_name_minus_type"></div>
    <div class="author-str">Mihaela Rosca &middot; Yan Wu &middot; Chongli Qin &middot; Benoit Dherin</div>
    <div class="author-str higher"></div>
    <div class="text-muted touchup-date-div" id="touchup-date-event-EYrRzKPinA"></div>

    <a href="paper_pages/EYrRzKPinA.html">
        <img src="http://img.youtube.com/vi/UKHCH8ZdH1Y/0.jpg" class="social-img-thumb rounded" alt="thumbnail"/>
    </a>

    <div class="abstract-section">
        <div>
            <a id="abstract-link-EYrRzKPinA" class="abstract-link" data-toggle="collapse"
               href="#collapse-event-abstract-EYrRzKPinA" role="button"
               aria-expanded="false" aria-controls="collapse-event-abstract-EYrRzKPinA">
                Abstract <i id="caret-EYrRzKPinA" class="fas fa-caret-right"></i>
            </a>
        </div>
    </div>

    <div class="collapse" id="collapse-event-abstract-EYrRzKPinA">
        <div class="abstract-display">
            <p>The recipe behind the success of deep learning has been the combination of neural networks and gradient-based optimization. Understanding the behavior of gradient descent however, and particularly its instability, has lagged behind its empirical success. To add to the theoretical tools available to study gradient descent we propose the principal flow (PF), a continuous time flow that approximates gradient descent dynamics. To our knowledge, the PF is the only continuous flow that captures the divergent and oscillatory behaviors of gradient descent, including escaping local minima and saddle points. Through its dependence on the eigendecomposition of the Hessian the PF sheds light on the recently observed edge of stability phenomena in deep learning. Using our new understanding of instability we propose a learning rate adaptation method which enables us to control the trade-off between training stability and test set evaluation performance.</p>
        </div>
    </div>
</div>

<div class="displaycards touchup-date" id="event-K6g4MbAC1r">
    <div style="width:80%;margin:auto;">
        <a class="small-title" href="paper_pages/K6g4MbAC1r.html">Investigating Action Encodings in Recurrent Neural Networks in Reinforcement Learning</a>
    </div>
    <div class="type_display_name_minus_type"></div>
    <div class="author-str">Matthew Kyle Schlegel &middot; Volodymyr Tkachuk &middot; Adam M White &middot; Martha White</div>
    <div class="author-str higher"></div>
    <div class="text-muted touchup-date-div" id="touchup-date-event-K6g4MbAC1r"></div>

    <a href="paper_pages/K6g4MbAC1r.html">
        <img src="http://img.youtube.com/vi/83vBK8DIdEY/0.jpg" class="social-img-thumb rounded" alt="thumbnail"/>
    </a>

    <div class="abstract-section">
        <div>
            <a id="abstract-link-K6g4MbAC1r" class="abstract-link" data-toggle="collapse"
               href="#collapse-event-abstract-K6g4MbAC1r" role="button"
               aria-expanded="false" aria-controls="collapse-event-abstract-K6g4MbAC1r">
                Abstract <i id="caret-K6g4MbAC1r" class="fas fa-caret-right"></i>
            </a>
        </div>
    </div>

    <div class="collapse" id="collapse-event-abstract-K6g4MbAC1r">
        <div class="abstract-display">
            <p>Building and maintaining state to learn policies and value functions is critical for deploying reinforcement learning (RL) agents in the real world. Recurrent neural networks (RNNs) have become a key point of interest for the state-building problem, and several large-scale reinforcement learning agents incorporate recurrent networks. While RNNs have become a mainstay in many RL applications, many key design choices and implementation details responsible for performance improvements are often not reported. In this work, we discuss one axis on which RNN architectures can be (and have been) modified for use in RL. Specifically, we look at how action information can be incorporated into the state update function of a recurrent cell. We discuss several choices in using action information and empirically evaluate the resulting architectures on a set of illustrative domains. Finally, we discuss future work in developing recurrent cells and discuss challenges specific to the RL setting.</p>
        </div>
    </div>
</div>

<div class="displaycards touchup-date" id="event-d3rHk4VAf0">
    <div style="width:80%;margin:auto;">
        <a class="small-title" href="paper_pages/d3rHk4VAf0.html">A Ranking Game for Imitation Learning</a>
    </div>
    <div class="type_display_name_minus_type"></div>
    <div class="author-str">Harshit Sikchi &middot; Akanksha Saran &middot; Wonjoon Goo &middot; Scott Niekum</div>
    <div class="author-str higher"></div>
    <div class="text-muted touchup-date-div" id="touchup-date-event-d3rHk4VAf0"></div>

    <a href="paper_pages/d3rHk4VAf0.html">
        <img src="http://img.youtube.com/vi/gTf8WoYUOH8/0.jpg" class="social-img-thumb rounded" alt="thumbnail"/>
    </a>

    <div class="abstract-section">
        <div>
            <a id="abstract-link-d3rHk4VAf0" class="abstract-link" data-toggle="collapse"
               href="#collapse-event-abstract-d3rHk4VAf0" role="button"
               aria-expanded="false" aria-controls="collapse-event-abstract-d3rHk4VAf0">
                Abstract <i id="caret-d3rHk4VAf0" class="fas fa-caret-right"></i>
            </a>
        </div>
    </div>

    <div class="collapse" id="collapse-event-abstract-d3rHk4VAf0">
        <div class="abstract-display">
            <p>We propose a new framework for imitation learning---treating imitation as a two-player ranking-based game between a policy and a reward. In this game, the reward agent learns to satisfy pairwise performance rankings between behaviors, while the policy agent learns to maximize this reward. In imitation learning, near-optimal expert data can be difficult to obtain, and even in the limit of infinite data cannot imply a total ordering over trajectories as preferences can. On the other hand, learning from preferences alone is challenging as a large number of preferences are required to infer a high-dimensional reward function, though preference data is typically much easier to collect than expert demonstrations. The classical inverse reinforcement learning (IRL) formulation learns from expert demonstrations but provides no mechanism to incorporate learning from offline preferences and vice versa. We instantiate the proposed ranking-game framework with a novel ranking loss giving an algorithm that can simultaneously learn from expert demonstrations and preferences, gaining the advantages of both modalities. Our experiments show that the proposed method achieves state-of-the-art sample efficiency and can solve previously unsolvable tasks in the Learning from Observation (LfO) setting.</p>
        </div>
    </div>
</div>

<div class="displaycards touchup-date" id="event-EiX2L4sDPG">
    <div style="width:80%;margin:auto;">
        <a class="small-title" href="paper_pages/EiX2L4sDPG.html">VN-Transformer: Rotation-Equivariant Attention for Vector Neurons</a>
    </div>
    <div class="type_display_name_minus_type"></div>
    <div class="author-str">Serge Assaad &middot; Carlton Downey &middot; Rami Al-Rfou' &middot; Nigamaa Nayakanti &middot; Benjamin Sapp</div>
    <div class="author-str higher"></div>
    <div class="text-muted touchup-date-div" id="touchup-date-event-EiX2L4sDPG"></div>

    <a href="paper_pages/EiX2L4sDPG.html">
        <img src="http://img.youtube.com/vi/1KrPzUKwSL8/0.jpg" class="social-img-thumb rounded" alt="thumbnail"/>
    </a>

    <div class="abstract-section">
        <div>
            <a id="abstract-link-EiX2L4sDPG" class="abstract-link" data-toggle="collapse"
               href="#collapse-event-abstract-EiX2L4sDPG" role="button"
               aria-expanded="false" aria-controls="collapse-event-abstract-EiX2L4sDPG">
                Abstract <i id="caret-EiX2L4sDPG" class="fas fa-caret-right"></i>
            </a>
        </div>
    </div>

    <div class="collapse" id="collapse-event-abstract-EiX2L4sDPG">
        <div class="abstract-display">
            <p>Rotation equivariance is a desirable property in many practical applications such as motion forecasting and 3D perception, where it can offer benefits like sample efficiency, better generalization, and robustness to input perturbations.
Vector Neurons (VN) is a recently developed framework offering a simple yet effective approach for deriving rotation-equivariant analogs of standard machine learning operations by extending one-dimensional scalar neurons to three-dimensional "vector neurons."
We introduce a novel "VN-Transformer" architecture to address several shortcomings of the current VN models. Our contributions are:
(i) we derive a rotation-equivariant attention mechanism which eliminates the need for the heavy feature preprocessing required by the original Vector Neurons models; (ii) we extend the VN framework to support non-spatial attributes, expanding the applicability of these models to real-world datasets; (iii) we derive a rotation-equivariant mechanism for multi-scale reduction of point-cloud resolution, greatly speeding up inference and training; (iv) we show that small tradeoffs in equivariance ($\epsilon$-approximate equivariance) can be used to obtain large improvements in numerical stability and training robustness on accelerated hardware, and we bound the propagation of equivariance violations in our models.
Finally, we apply our VN-Transformer to 3D shape classification and motion forecasting with compelling results.</p>
        </div>
    </div>
</div>

<div class="displaycards touchup-date" id="event-myjAVQrRxS">
    <div style="width:80%;margin:auto;">
        <a class="small-title" href="paper_pages/myjAVQrRxS.html">Dropped Scheduled Task: Mitigating Negative Transfer in Multi-task Learning using Dynamic Task Dropping</a>
    </div>
    <div class="type_display_name_minus_type"></div>
    <div class="author-str">Aakarsh Malhotra &middot; Mayank Vatsa &middot; Richa Singh</div>
    <div class="author-str higher"></div>
    <div class="text-muted touchup-date-div" id="touchup-date-event-myjAVQrRxS"></div>

    <a href="paper_pages/myjAVQrRxS.html">
        <img src="http://img.youtube.com/vi/NZrnPHDFNTM/0.jpg" class="social-img-thumb rounded" alt="thumbnail"/>
    </a>

    <div class="abstract-section">
        <div>
            <a id="abstract-link-myjAVQrRxS" class="abstract-link" data-toggle="collapse"
               href="#collapse-event-abstract-myjAVQrRxS" role="button"
               aria-expanded="false" aria-controls="collapse-event-abstract-myjAVQrRxS">
                Abstract <i id="caret-myjAVQrRxS" class="fas fa-caret-right"></i>
            </a>
        </div>
    </div>

    <div class="collapse" id="collapse-event-abstract-myjAVQrRxS">
        <div class="abstract-display">
            <p>In Multi-Task Learning (MTL), K distinct tasks are jointly optimized. With the varying nature and complexities of tasks, few tasks might dominate learning. For other tasks, their respective performances may get compromised due to a negative transfer from dominant tasks. We propose a Dropped-Scheduled Task (DST) algorithm, which probabilistically “drops” specific tasks during joint optimization while scheduling others to reduce negative transfer. For each task, a scheduling probability is decided based on four different metrics: (i) task depth, (ii) number of ground-truth samples per task, (iii) amount of training completed, and (iv) task stagnancy. Based on the scheduling probability, specific tasks get joint computation cycles while others are “dropped”. To demonstrate the effectiveness of the proposed DST algorithm, we perform multi-task learning on three applications and two architectures. Across unilateral (single input) and bilateral (multiple input) multi-task net- works, the chosen applications are (a) face (AFLW), (b) fingerprint (IIITD MOLF, MUST, and NIST SD27), and (c) character recognition (Omniglot) applications. Experimental results show that the proposed DST algorithm has the minimum negative transfer and overall least errors across different state-of-the-art algorithms and tasks.</p>
        </div>
    </div>
</div>

<div class="displaycards touchup-date" id="event-TNocbXm5MZ">
    <div style="width:80%;margin:auto;">
        <a class="small-title" href="paper_pages/TNocbXm5MZ.html">Guaranteed Discovery of Control-Endogenous Latent States with Multi-Step Inverse Models</a>
    </div>
    <div class="type_display_name_minus_type"></div>
    <div class="author-str">Alex Lamb &middot; Riashat Islam &middot; Yonathan Efroni &middot; Aniket Rajiv Didolkar &middot; Dipendra Misra &middot; Dylan J Foster &middot; Lekan P Molu &middot; Rajan Chari &middot; Akshay Krishnamurthy &middot; John Langford</div>
    <div class="author-str higher"></div>
    <div class="text-muted touchup-date-div" id="touchup-date-event-TNocbXm5MZ"></div>

    <a href="paper_pages/TNocbXm5MZ.html">
        <img src="http://img.youtube.com/vi/prS5EG9dLVg/0.jpg" class="social-img-thumb rounded" alt="thumbnail"/>
    </a>

    <div class="abstract-section">
        <div>
            <a id="abstract-link-TNocbXm5MZ" class="abstract-link" data-toggle="collapse"
               href="#collapse-event-abstract-TNocbXm5MZ" role="button"
               aria-expanded="false" aria-controls="collapse-event-abstract-TNocbXm5MZ">
                Abstract <i id="caret-TNocbXm5MZ" class="fas fa-caret-right"></i>
            </a>
        </div>
    </div>

    <div class="collapse" id="collapse-event-abstract-TNocbXm5MZ">
        <div class="abstract-display">
            <p>In many sequential decision-making tasks, the agent is not able to model the full complexity of the world, which consists of multitudes of relevant and irrelevant information. For example, a person walking along a city street who tries to model all aspects of the world would quickly be overwhelmed by a multitude of shops, cars, and people moving in and out of view, each following their own complex and inscrutable dynamics.  Is it possible to turn the agent's firehose of sensory information into a minimal latent state that is both necessary and sufficient for an agent to successfully act in the world? We formulate this question concretely, and propose the Agent Control-Endogenous State Discovery algorithm (AC-State), which has theoretical guarantees and is practically demonstrated to discover the minimal control-endogenous latent state which contains all of the information necessary for controlling the agent, while fully discarding all irrelevant information.    This algorithm consists of a multi-step inverse model (predicting actions from distant observations) with an information bottleneck.  AC-State enables localization, exploration, and navigation without reward or demonstrations.  We demonstrate the discovery of the control-endogenous latent state in three domains: localizing a robot arm with distractions (e.g., changing lighting conditions and background), exploring a maze alongside other agents, and navigating in the Matterport house simulator.  </p>
        </div>
    </div>
</div>

<div class="displaycards touchup-date" id="event-JwDpZSv3yz">
    <div style="width:80%;margin:auto;">
        <a class="small-title" href="paper_pages/JwDpZSv3yz.html">SPADE: Semi-supervised Anomaly Detection under Distribution Mismatch</a>
    </div>
    <div class="type_display_name_minus_type"></div>
    <div class="author-str">Jinsung Yoon &middot; Kihyuk Sohn &middot; Chun-Liang Li &middot; Sercan O Arik &middot; Tomas Pfister</div>
    <div class="author-str higher"></div>
    <div class="text-muted touchup-date-div" id="touchup-date-event-JwDpZSv3yz"></div>

    <a href="paper_pages/JwDpZSv3yz.html">
        <img src="https://drive.google.com/thumbnail?id=1UvqBpvcPw3XQISI5ZI-upYdPlOUNqcvi" class="social-img-thumb rounded" alt="thumbnail"/>
    </a>

    <div class="abstract-section">
        <div>
            <a id="abstract-link-JwDpZSv3yz" class="abstract-link" data-toggle="collapse"
               href="#collapse-event-abstract-JwDpZSv3yz" role="button"
               aria-expanded="false" aria-controls="collapse-event-abstract-JwDpZSv3yz">
                Abstract <i id="caret-JwDpZSv3yz" class="fas fa-caret-right"></i>
            </a>
        </div>
    </div>

    <div class="collapse" id="collapse-event-abstract-JwDpZSv3yz">
        <div class="abstract-display">
            <p>Semi-supervised anomaly detection is a common problem, as often the datasets containing anomalies are partially labeled. We propose a canonical framework: Semi-supervised Pseudo-labeler Anomaly Detection with Ensembling (SPADE) that isn't limited by the assumption that labeled and unlabeled data come from the same distribution. Indeed, the assumption is often violated in many applications -- for example, the labeled data may contain only anomalies unlike unlabeled data, or unlabeled data may contain different types of anomalies, or labeled data may contain only `easy-to-label' samples. SPADE utilizes an ensemble of one class classifiers as the pseudo-labeler to improve the robustness of pseudo-labeling with distribution mismatch. Partial matching is proposed to automatically select the critical hyper-parameters for pseudo-labeling without validation data, which is crucial with limited labeled data. SPADE shows state-of-the-art semi-supervised anomaly detection performance across a wide range of scenarios with distribution mismatch in both tabular and image domains. In some common real-world settings such as model facing new types of unlabeled anomalies, SPADE outperforms the state-of-the-art alternatives by 5% AUC in average.</p>
        </div>
    </div>
</div>

<div class="displaycards touchup-date" id="event-gZna3IiGfl">
    <div style="width:80%;margin:auto;">
        <a class="small-title" href="paper_pages/gZna3IiGfl.html">Mean-field analysis for heavy ball methods: Dropout-stability, connectivity, and global convergence</a>
    </div>
    <div class="type_display_name_minus_type"></div>
    <div class="author-str">Diyuan Wu &middot; Vyacheslav Kungurtsev &middot; Marco Mondelli</div>
    <div class="author-str higher"></div>
    <div class="text-muted touchup-date-div" id="touchup-date-event-gZna3IiGfl"></div>

    <a href="paper_pages/gZna3IiGfl.html">
        <img src="tmlr_logo.jpeg" class="social-img-thumb rounded" alt="thumbnail"/>
    </a>

    <div class="abstract-section">
        <div>
            <a id="abstract-link-gZna3IiGfl" class="abstract-link" data-toggle="collapse"
               href="#collapse-event-abstract-gZna3IiGfl" role="button"
               aria-expanded="false" aria-controls="collapse-event-abstract-gZna3IiGfl">
                Abstract <i id="caret-gZna3IiGfl" class="fas fa-caret-right"></i>
            </a>
        </div>
    </div>

    <div class="collapse" id="collapse-event-abstract-gZna3IiGfl">
        <div class="abstract-display">
            <p>The stochastic heavy ball method (SHB), also known as stochastic gradient descent (SGD) with Polyak's momentum, is widely used in training neural networks. However, despite the remarkable success of such algorithm in practice, its theoretical characterization remains limited. In this paper, we focus on neural networks with two and three layers and provide a rigorous understanding of the properties of the solutions found by SHB: \emph{(i)} stability after dropping out part of the neurons, \emph{(ii)} connectivity along a low-loss path, and \emph{(iii)} convergence to the global optimum.
To achieve this goal, we take a mean-field view and relate the SHB dynamics to a certain partial differential equation in the limit of large network widths. This mean-field perspective has inspired a recent line of work focusing on SGD while, in contrast, our paper considers an algorithm with momentum. More specifically, after proving existence and uniqueness of the limit differential equations, we show convergence to the global optimum and give a quantitative bound between the mean-field limit and the SHB dynamics of a finite-width network. Armed with this last bound, we are able to establish the dropout-stability and connectivity of SHB solutions. </p>
        </div>
    </div>
</div>

<div class="displaycards touchup-date" id="event-WN1O2MJDST">
    <div style="width:80%;margin:auto;">
        <a class="small-title" href="paper_pages/WN1O2MJDST.html">Learn, Unlearn and Relearn: An Online Learning Paradigm for Deep Neural Networks</a>
    </div>
    <div class="type_display_name_minus_type"></div>
    <div class="author-str">Vijaya Raghavan T Ramkumar &middot; Elahe Arani &middot; Bahram Zonooz</div>
    <div class="author-str higher"></div>
    <div class="text-muted touchup-date-div" id="touchup-date-event-WN1O2MJDST"></div>

    <a href="paper_pages/WN1O2MJDST.html">
        <img src="http://img.youtube.com/vi/efulZvJBZ04/0.jpg" class="social-img-thumb rounded" alt="thumbnail"/>
    </a>

    <div class="abstract-section">
        <div>
            <a id="abstract-link-WN1O2MJDST" class="abstract-link" data-toggle="collapse"
               href="#collapse-event-abstract-WN1O2MJDST" role="button"
               aria-expanded="false" aria-controls="collapse-event-abstract-WN1O2MJDST">
                Abstract <i id="caret-WN1O2MJDST" class="fas fa-caret-right"></i>
            </a>
        </div>
    </div>

    <div class="collapse" id="collapse-event-abstract-WN1O2MJDST">
        <div class="abstract-display">
            <p>Deep neural networks (DNNs) are often trained on the premise that the complete training data set is provided ahead of time. However, in real-world scenarios, data often arrive in chunks over time. This leads to important considerations about the optimal strategy for training DNNs, such as whether to fine-tune them with each chunk of incoming data (warm-start) or to retrain them from scratch with the entire corpus of data whenever a new chunk is available. While employing the latter for training can be resource-intensive, recent work has pointed out the lack of generalization in warm-start models. Therefore, to strike a balance between efficiency and generalization, we introduce "Learn, Unlearn, and Relearn (LURE)" an online learning paradigm for DNNs. LURE interchanges between the unlearning phase, which selectively forgets the undesirable information in the model through weight reinitialization in a data-dependent manner, and the relearning phase, which emphasizes learning on generalizable features. We show that our training paradigm provides consistent performance gains across datasets in both classification and few-shot settings. We further show that it leads to more robust and well-calibrated models.</p>
        </div>
    </div>
</div>

<div class="displaycards touchup-date" id="event-3gfpBR1ncr">
    <div style="width:80%;margin:auto;">
        <a class="small-title" href="paper_pages/3gfpBR1ncr.html">On Characterizing the Trade-off in Invariant Representation Learning</a>
    </div>
    <div class="type_display_name_minus_type"></div>
    <div class="author-str">Bashir Sadeghi &middot; Sepehr Dehdashtian &middot; Vishnu Boddeti</div>
    <div class="author-str higher"></div>
    <div class="text-muted touchup-date-div" id="touchup-date-event-3gfpBR1ncr"></div>

    <a href="paper_pages/3gfpBR1ncr.html">
        <img src="http://img.youtube.com/vi/bjeLIWoiTT8/0.jpg" class="social-img-thumb rounded" alt="thumbnail"/>
    </a>

    <div class="abstract-section">
        <div>
            <a id="abstract-link-3gfpBR1ncr" class="abstract-link" data-toggle="collapse"
               href="#collapse-event-abstract-3gfpBR1ncr" role="button"
               aria-expanded="false" aria-controls="collapse-event-abstract-3gfpBR1ncr">
                Abstract <i id="caret-3gfpBR1ncr" class="fas fa-caret-right"></i>
            </a>
        </div>
    </div>

    <div class="collapse" id="collapse-event-abstract-3gfpBR1ncr">
        <div class="abstract-display">
            <p>Many applications of representation learning, such as privacy preservation, algorithmic fairness, and domain adaptation, desire explicit control over semantic information being discarded. This goal is formulated as satisfying two objectives: maximizing utility for predicting a target attribute while simultaneously being invariant (independent) to a known semantic attribute. Solutions to invariant representation learning (IRepL) problems lead to a trade-off between utility and invariance when they are competing. While existing works study bounds on this trade-off, two questions remain outstanding: 1) What is the exact trade-off between utility and invariance? and 2) What are the encoders (mapping the data to a representation) that achieve the trade-off, and how can we estimate it from training data? This paper addresses these questions for IRepLs in reproducing kernel Hilbert spaces (RKHS)s. Under the assumption that the distribution of a low-dimensional projection of high-dimensional data is approximately normal, we derive a closed-form solution for the global optima of the underlying optimization problem for encoders in RKHSs. This yields closed formulae for a near-optimal trade-off, corresponding optimal representation dimensionality, and the corresponding encoder(s). We also numerically quantify the trade-off on representative problems and compare them to those achieved by baseline IRepL algorithms.</p>
        </div>
    </div>
</div>

<div class="displaycards touchup-date" id="event-P9Cj6RJmN2">
    <div style="width:80%;margin:auto;">
        <a class="small-title" href="paper_pages/P9Cj6RJmN2.html">A Stochastic Optimization Framework for Fair Risk Minimization</a>
    </div>
    <div class="type_display_name_minus_type"></div>
    <div class="author-str">Andrew Lowy &middot; Sina Baharlouei &middot; Rakesh Pavan &middot; Meisam Razaviyayn &middot; Ahmad Beirami</div>
    <div class="author-str higher"></div>
    <div class="text-muted touchup-date-div" id="touchup-date-event-P9Cj6RJmN2"></div>

    <a href="paper_pages/P9Cj6RJmN2.html">
        <img src="tmlr_logo.jpeg" class="social-img-thumb rounded" alt="thumbnail"/>
    </a>

    <div class="abstract-section">
        <div>
            <a id="abstract-link-P9Cj6RJmN2" class="abstract-link" data-toggle="collapse"
               href="#collapse-event-abstract-P9Cj6RJmN2" role="button"
               aria-expanded="false" aria-controls="collapse-event-abstract-P9Cj6RJmN2">
                Abstract <i id="caret-P9Cj6RJmN2" class="fas fa-caret-right"></i>
            </a>
        </div>
    </div>

    <div class="collapse" id="collapse-event-abstract-P9Cj6RJmN2">
        <div class="abstract-display">
            <p>Despite the success of large-scale empirical risk minimization (ERM) at achieving high accuracy across a variety of machine learning tasks, fair ERM is hindered by the incompatibility of fairness constraints with stochastic optimization. We consider the problem of fair classification with discrete sensitive attributes and potentially large models and data sets, requiring stochastic solvers. Existing in-processing fairness algorithms are either impractical in the large-scale setting because they require large batches of data at each iteration or they are not guaranteed to converge. In this paper, we develop the first stochastic in-processing fairness algorithm with guaranteed convergence. For demographic parity, equalized odds, and equal opportunity notions of fairness, we provide slight variations of our algorithm–called FERMI–and prove that each of these variations converges in stochastic optimization with any batch size. Empirically, we show that FERMI is amenable to stochastic solvers with multiple (non-binary) sensitive attributes and non-binary targets, performing well even with minibatch size as small as one. Extensive experiments show that FERMI achieves the most favorable tradeoffs between fairness violation and test accuracy across all tested setups compared with state-of-the-art baselines for demographic parity, equalized odds, equal opportunity. These benefits are especially significant with small batch sizes and for non-binary classification with large number of sensitive attributes, making FERMI a practical, scalable fairness algorithm. The code for all of the experiments in this paper is available at:
https://github.com/optimization-for-data-driven-science/FERMI</p>
        </div>
    </div>
</div>

<div class="displaycards touchup-date" id="event-VW4IrC0n0M">
    <div style="width:80%;margin:auto;">
        <a class="small-title" href="paper_pages/VW4IrC0n0M.html">An approximate sampler for energy-based models with divergence diagnostics</a>
    </div>
    <div class="type_display_name_minus_type"></div>
    <div class="author-str">Bryan Eikema &middot; Germán Kruszewski &middot; Christopher R Dance &middot; Hady Elsahar &middot; Marc Dymetman</div>
    <div class="author-str higher"></div>
    <div class="text-muted touchup-date-div" id="touchup-date-event-VW4IrC0n0M"></div>

    <a href="paper_pages/VW4IrC0n0M.html">
        <img src="tmlr_logo.jpeg" class="social-img-thumb rounded" alt="thumbnail"/>
    </a>

    <div class="abstract-section">
        <div>
            <a id="abstract-link-VW4IrC0n0M" class="abstract-link" data-toggle="collapse"
               href="#collapse-event-abstract-VW4IrC0n0M" role="button"
               aria-expanded="false" aria-controls="collapse-event-abstract-VW4IrC0n0M">
                Abstract <i id="caret-VW4IrC0n0M" class="fas fa-caret-right"></i>
            </a>
        </div>
    </div>

    <div class="collapse" id="collapse-event-abstract-VW4IrC0n0M">
        <div class="abstract-display">
            <p>Energy-based models (EBMs) allow flexible specifications of probability distributions. However, sampling from EBMs is non-trivial, usually requiring approximate techniques such as Markov chain Monte Carlo (MCMC). A major downside of MCMC sampling is that it is often impossible to compute the divergence of the sampling distribution from the target distribution: therefore, the quality of the samples cannot be guaranteed. Here, we introduce quasi-rejection sampling (QRS), a simple extension of rejection sampling that performs approximate sampling, but, crucially, does provide divergence diagnostics (in terms of f-divergences, such as KL divergence and total variation distance). We apply QRS to sampling from discrete EBMs over text for controlled generation. We show that we can sample from such EBMs with arbitrary precision in exchange for sampling efficiency and quantify the trade-off between the two by means of the aforementioned diagnostics. 
</p>
        </div>
    </div>
</div>

<div class="displaycards touchup-date" id="event-LHAbHkt6Aq">
    <div style="width:80%;margin:auto;">
        <a class="small-title" href="paper_pages/LHAbHkt6Aq.html">A Crisis In Simulation-Based Inference? Beware, Your Posterior Approximations Can Be Unfaithful</a>
    </div>
    <div class="type_display_name_minus_type"></div>
    <div class="author-str">Joeri Hermans &middot; Arnaud Delaunoy &middot; François Rozet &middot; Antoine Wehenkel &middot; Volodimir Begy &middot; Gilles Louppe</div>
    <div class="author-str higher"></div>
    <div class="text-muted touchup-date-div" id="touchup-date-event-LHAbHkt6Aq"></div>

    <a href="paper_pages/LHAbHkt6Aq.html">
        <img src="http://img.youtube.com/vi/sLM0JBbSjLw/0.jpg" class="social-img-thumb rounded" alt="thumbnail"/>
    </a>

    <div class="abstract-section">
        <div>
            <a id="abstract-link-LHAbHkt6Aq" class="abstract-link" data-toggle="collapse"
               href="#collapse-event-abstract-LHAbHkt6Aq" role="button"
               aria-expanded="false" aria-controls="collapse-event-abstract-LHAbHkt6Aq">
                Abstract <i id="caret-LHAbHkt6Aq" class="fas fa-caret-right"></i>
            </a>
        </div>
    </div>

    <div class="collapse" id="collapse-event-abstract-LHAbHkt6Aq">
        <div class="abstract-display">
            <p>We present extensive empirical evidence showing that current Bayesian simulation-based inference algorithms can produce computationally unfaithful posterior approximations. Our results show that all benchmarked algorithms -- (S)NPE, (S)NRE, SNL and variants of ABC -- can yield overconfident posterior approximations, which makes them unreliable for scientific use cases and falsificationist inquiry. Failing to address this issue may reduce the range of applicability of simulation-based inference. For this reason, we argue that research efforts should be made towards theoretical and methodological developments of conservative approximate inference algorithms and present research directions towards this objective. In this regard, we show empirical evidence that ensembling posterior surrogates provides more reliable approximations and mitigates the issue.</p>
        </div>
    </div>
</div>

<div class="displaycards touchup-date" id="event-QaDevCcmcg">
    <div style="width:80%;margin:auto;">
        <a class="small-title" href="paper_pages/QaDevCcmcg.html">Uncertainty-Based Active Learning for Reading Comprehension</a>
    </div>
    <div class="type_display_name_minus_type"></div>
    <div class="author-str">Jing Wang &middot; Jie Shen &middot; Xiaofei Ma &middot; Andrew Arnold</div>
    <div class="author-str higher"></div>
    <div class="text-muted touchup-date-div" id="touchup-date-event-QaDevCcmcg"></div>

    <a href="paper_pages/QaDevCcmcg.html">
        <img src="http://img.youtube.com/vi/oRrMip1MWbY/0.jpg" class="social-img-thumb rounded" alt="thumbnail"/>
    </a>

    <div class="abstract-section">
        <div>
            <a id="abstract-link-QaDevCcmcg" class="abstract-link" data-toggle="collapse"
               href="#collapse-event-abstract-QaDevCcmcg" role="button"
               aria-expanded="false" aria-controls="collapse-event-abstract-QaDevCcmcg">
                Abstract <i id="caret-QaDevCcmcg" class="fas fa-caret-right"></i>
            </a>
        </div>
    </div>

    <div class="collapse" id="collapse-event-abstract-QaDevCcmcg">
        <div class="abstract-display">
            <p>Recent years have witnessed a surge of successful applications of machine reading comprehension. Of central importance to these tasks is the availability of massive amount of labeled data, which facilitates training of large-scale neural networks. However, in many real-world problems, annotated data are expensive to gather not only because of time cost and budget, but also  of certain domain-specific restrictions such as privacy for healthcare data. In this regard, we propose an uncertainty-based active learning algorithm for reading comprehension, which interleaves data annotation and model updating to mitigate the demand of labeling. Our key techniques are two-fold: 1) an unsupervised uncertainty-based sampling scheme that queries the labels of the most informative instances with respect to the currently learned model; and 2) an adaptive loss minimization paradigm that simultaneously fits the data and controls the degree of model updating. We demonstrate on  benchmark datasets that 25% less labeled samples suffice to guarantee similar, or even improved performance. Our results show strong evidence that for label-demanding scenarios, the proposed approach offers a practical guide on data collection and model training. </p>
        </div>
    </div>
</div>

<div class="displaycards touchup-date" id="event-k4iWTEdUSF">
    <div style="width:80%;margin:auto;">
        <a class="small-title" href="paper_pages/k4iWTEdUSF.html">Fast and Accurate Spreading Process Temporal Scale Estimation</a>
    </div>
    <div class="type_display_name_minus_type"></div>
    <div class="author-str">Abram Magner &middot; Carolyn S Kaminski &middot; Petko Bogdanov</div>
    <div class="author-str higher"></div>
    <div class="text-muted touchup-date-div" id="touchup-date-event-k4iWTEdUSF"></div>

    <a href="paper_pages/k4iWTEdUSF.html">
        <img src="tmlr_logo.jpeg" class="social-img-thumb rounded" alt="thumbnail"/>
    </a>

    <div class="abstract-section">
        <div>
            <a id="abstract-link-k4iWTEdUSF" class="abstract-link" data-toggle="collapse"
               href="#collapse-event-abstract-k4iWTEdUSF" role="button"
               aria-expanded="false" aria-controls="collapse-event-abstract-k4iWTEdUSF">
                Abstract <i id="caret-k4iWTEdUSF" class="fas fa-caret-right"></i>
            </a>
        </div>
    </div>

    <div class="collapse" id="collapse-event-abstract-k4iWTEdUSF">
        <div class="abstract-display">
            <p>Spreading processes on graphs arise in a host of application domains, from the study of online social networks to viral marketing to epidemiology. Various discrete-time probabilistic models for spreading processes have been proposed. These are used for downstream statistical estimation and prediction problems, often involving messages or other information that is transmitted along with infections caused by the process. These models generally model cascade behavior at a small time scale but are insufficiently flexible to model cascades that exhibit intermittent behavior governed by multiple scales.  We argue that the presence of such time scales that are unaccounted for by a cascade model can result in degradation of performance of models on downstream statistical and time-sensitive optimization tasks.  To address these issues, we formulate a model that incorporates multiple temporal scales of cascade behavior.  This model is parameterized by a \emph{clock}, which encodes the times at which sessions of cascade activity start.  These sessions are themselves governed by a small-scale cascade model, such as the discretized independent cascade (IC) model.  Estimation of the multiscale cascade model parameters leads to the problem of \emph{clock estimation} in terms of a natural distortion measure that we formulate.  Our framework is inspired by the optimization problem posed by DiTursi et al, 2017, which can be seen as providing one possible estimator (a maximum-proxy-likelihood estimator) for the parameters of our generative model. We give a clock estimation algorithm, which we call FastClock, that runs in linear time in the size of its input and is provably statistically accurate for a broad range of model parameters when cascades are generated from any spreading process model with well-concentrated session infection set sizes and when the underlying graph is at least in the semi-sparse regime.  We exemplify our algorithm for the case where the small-scale model is the discretized independent cascade process and extend substantially to processes whose infection set sizes satisfy a general martingale difference property. We further evaluate the performance of FastClock empirically in comparison to the state of the art estimator from DiTursi et al, 2017.  We find that in a broad parameter range on synthetic networks and on a real network, our algorithm substantially outperforms that algorithm in terms of both running time and accuracy.  In all cases, our algorithm's running time is asymptotically lower than that of the baseline.
</p>
        </div>
    </div>
</div>

<div class="displaycards touchup-date" id="event-NmTMc3uD1G">
    <div style="width:80%;margin:auto;">
        <a class="small-title" href="paper_pages/NmTMc3uD1G.html">Modeling Object Dissimilarity for Deep Saliency Prediction</a>
    </div>
    <div class="type_display_name_minus_type"></div>
    <div class="author-str">Bahar Aydemir &middot; Deblina Bhattacharjee &middot; Tong Zhang &middot; Seungryong Kim &middot; Mathieu Salzmann &middot; Sabine Süsstrunk</div>
    <div class="author-str higher"></div>
    <div class="text-muted touchup-date-div" id="touchup-date-event-NmTMc3uD1G"></div>

    <a href="paper_pages/NmTMc3uD1G.html">
        <img src="tmlr_logo.jpeg" class="social-img-thumb rounded" alt="thumbnail"/>
    </a>

    <div class="abstract-section">
        <div>
            <a id="abstract-link-NmTMc3uD1G" class="abstract-link" data-toggle="collapse"
               href="#collapse-event-abstract-NmTMc3uD1G" role="button"
               aria-expanded="false" aria-controls="collapse-event-abstract-NmTMc3uD1G">
                Abstract <i id="caret-NmTMc3uD1G" class="fas fa-caret-right"></i>
            </a>
        </div>
    </div>

    <div class="collapse" id="collapse-event-abstract-NmTMc3uD1G">
        <div class="abstract-display">
            <p>Saliency prediction has made great strides over the past two decades, with current techniques modeling low-level information, such as color, intensity and size contrasts, and high-level ones, such as attention and gaze direction for entire objects. Despite this, these methods fail to account for the dissimilarity between objects, which affects human visual attention. In this paper, we introduce a detection-guided saliency prediction network that explicitly models the differences between multiple objects, such as their appearance and size dissimilarities. Our approach allows us to fuse our object dissimilarities with features extracted by any deep saliency prediction network. As evidenced by our experiments, this consistently boosts the accuracy of the baseline networks, enabling us to outperform the state-of-the-art models on three saliency benchmarks, namely SALICON, MIT300 and CAT2000. Our project page is at https://github.com/IVRL/DisSal.</p>
        </div>
    </div>
</div>

<div class="displaycards touchup-date" id="event-Lgs5pQ1v30">
    <div style="width:80%;margin:auto;">
        <a class="small-title" href="paper_pages/Lgs5pQ1v30.html">FedShuffle:  Recipes for Better Use of Local Work in Federated Learning</a>
    </div>
    <div class="type_display_name_minus_type"></div>
    <div class="author-str">Samuel Horváth &middot; Maziar Sanjabi &middot; Lin Xiao &middot; Peter Richtárik &middot; Michael Rabbat</div>
    <div class="author-str higher"></div>
    <div class="text-muted touchup-date-div" id="touchup-date-event-Lgs5pQ1v30"></div>

    <a href="paper_pages/Lgs5pQ1v30.html">
        <img src="http://img.youtube.com/vi/jGVbs3Jl1K0/0.jpg" class="social-img-thumb rounded" alt="thumbnail"/>
    </a>

    <div class="abstract-section">
        <div>
            <a id="abstract-link-Lgs5pQ1v30" class="abstract-link" data-toggle="collapse"
               href="#collapse-event-abstract-Lgs5pQ1v30" role="button"
               aria-expanded="false" aria-controls="collapse-event-abstract-Lgs5pQ1v30">
                Abstract <i id="caret-Lgs5pQ1v30" class="fas fa-caret-right"></i>
            </a>
        </div>
    </div>

    <div class="collapse" id="collapse-event-abstract-Lgs5pQ1v30">
        <div class="abstract-display">
            <p>The practice of applying several local updates before aggregation across clients has been empirically shown to be a successful approach to overcoming the communication bottleneck in Federated Learning (FL). Such methods are usually implemented by having clients perform one or more epochs of local training per round while randomly reshuffling their finite dataset in each epoch. Data imbalance, where clients have different numbers of local training samples, is ubiquitous in FL applications, resulting in different clients performing different numbers of local updates in each round. In this work, we propose a general recipe, FedShuffle, that better utilizes the local updates in FL, especially in this regime encompassing random reshuffling and heterogeneity. FedShuffle is the first local update method with theoretical convergence guarantees that incorporates random reshuffling, data imbalance, and client sampling — features that are essential in large-scale cross-device FL. We present a comprehensive theoretical analysis of FedShuffle and show, both theoretically and empirically, that it does not suffer from the objective function mismatch that is present in FL methods that assume homogeneous updates in heterogeneous FL setups, such as FedAvg (McMahan et al., 2017). In addition, by combining the ingredients above, FedShuffle improves upon FedNova (Wang et al., 2020), which was previously proposed to solve this mismatch. Similar to Mime (Karimireddy et al., 2020), we show that FedShuffle with momentum variance reduction (Cutkosky & Orabona, 2019) improves upon non-local methods under a Hessian similarity assumption.</p>
        </div>
    </div>
</div>

<div class="displaycards touchup-date" id="event-MHOAEiTlen">
    <div style="width:80%;margin:auto;">
        <a class="small-title" href="paper_pages/MHOAEiTlen.html">DHA: End-to-End Joint Optimization of Data Augmentation Policy, Hyper-parameter and Architecture</a>
    </div>
    <div class="type_display_name_minus_type"></div>
    <div class="author-str">kaichen zhou &middot; Lanqing HONG &middot; Shoukang Hu &middot; Fengwei Zhou &middot; Binxin Ru &middot; Jiashi Feng &middot; Zhenguo Li</div>
    <div class="author-str higher"></div>
    <div class="text-muted touchup-date-div" id="touchup-date-event-MHOAEiTlen"></div>

    <a href="paper_pages/MHOAEiTlen.html">
        <img src="tmlr_logo.jpeg" class="social-img-thumb rounded" alt="thumbnail"/>
    </a>

    <div class="abstract-section">
        <div>
            <a id="abstract-link-MHOAEiTlen" class="abstract-link" data-toggle="collapse"
               href="#collapse-event-abstract-MHOAEiTlen" role="button"
               aria-expanded="false" aria-controls="collapse-event-abstract-MHOAEiTlen">
                Abstract <i id="caret-MHOAEiTlen" class="fas fa-caret-right"></i>
            </a>
        </div>
    </div>

    <div class="collapse" id="collapse-event-abstract-MHOAEiTlen">
        <div class="abstract-display">
            <p>Automated machine learning (AutoML) usually involves several crucial components, such as Data Augmentation (DA) policy, Hyper-Parameter Optimization (HPO), and Neural Architecture Search (NAS). Although many strategies have been developed for automating these components in separation, joint optimization of these components remains challenging due to the largely increased search dimension and the variant input types of each component. In parallel to this, the common practice of searching for the optimal architecture first and then retraining it before deployment in NAS often suffers from the low-performance correlation between the searching and retraining stages. An end-to-end solution that integrates the AutoML components and returns a ready-to-use model at the end of the search is desirable. In view of these, we propose DHA, which achieves joint optimization of Data augmentation policy, Hyper-parameter, and Architecture. Specifically, end-to-end NAS is achieved in a differentiable manner by optimizing a compressed lower-dimensional feature space, while DA policy and HPO are regarded as dynamic schedulers, which adapt themselves to the update of network parameters and network architecture at the same time. Experiments show that DHA achieves state-of-the-art (SOTA) results on various datasets and search spaces. To the best of our knowledge, we are the first to efficiently and jointly optimize DA policy, NAS, and HPO in an end-to-end manner without retraining.</p>
        </div>
    </div>
</div>

<div class="displaycards touchup-date" id="event-vUuHPRrWs2">
    <div style="width:80%;margin:auto;">
        <a class="small-title" href="paper_pages/vUuHPRrWs2.html">Practicality of generalization guarantees for unsupervised domain adaptation with neural networks</a>
    </div>
    <div class="type_display_name_minus_type"></div>
    <div class="author-str">Adam Breitholtz &middot; Fredrik Daniel Johansson</div>
    <div class="author-str higher"></div>
    <div class="text-muted touchup-date-div" id="touchup-date-event-vUuHPRrWs2"></div>

    <a href="paper_pages/vUuHPRrWs2.html">
        <img src="tmlr_logo.jpeg" class="social-img-thumb rounded" alt="thumbnail"/>
    </a>

    <div class="abstract-section">
        <div>
            <a id="abstract-link-vUuHPRrWs2" class="abstract-link" data-toggle="collapse"
               href="#collapse-event-abstract-vUuHPRrWs2" role="button"
               aria-expanded="false" aria-controls="collapse-event-abstract-vUuHPRrWs2">
                Abstract <i id="caret-vUuHPRrWs2" class="fas fa-caret-right"></i>
            </a>
        </div>
    </div>

    <div class="collapse" id="collapse-event-abstract-vUuHPRrWs2">
        <div class="abstract-display">
            <p>Understanding generalization is crucial to confidently engineer and deploy machine learning models, especially when deployment implies a shift in the data domain. 
For such domain adaptation problems, we seek generalization bounds which are tractably computable and tight. If these desiderata can be reached, the bounds can serve as guarantees for adequate performance in deployment.
However, in applications where deep neural networks are the models of choice, deriving results which fulfill these remains an unresolved challenge; most existing bounds are either vacuous or has non-estimable terms, even in favorable conditions.
In this work, we evaluate existing bounds from the literature with potential to satisfy our desiderata on domain adaptation image classification tasks, where deep neural networks are preferred. We find that all bounds are vacuous and that sample generalization terms account for much of the observed looseness, especially when these terms interact with measures of domain shift. To overcome this and arrive at the tightest possible results, we combine each bound with recent data-dependent PAC-Bayes analysis, greatly improving the guarantees. We find that, when domain overlap can be assumed, a simple importance weighting extension of previous work provides the tightest estimable bound. Finally, we study which terms dominate the bounds and identify possible directions for further improvement. </p>
        </div>
    </div>
</div>

<div class="displaycards touchup-date" id="event-Sh3RF9JowK">
    <div style="width:80%;margin:auto;">
        <a class="small-title" href="paper_pages/Sh3RF9JowK.html">Learning Algorithms for Markovian Bandits:\\Is Posterior Sampling more Scalable than Optimism?</a>
    </div>
    <div class="type_display_name_minus_type"></div>
    <div class="author-str">Nicolas Gast &middot; Bruno Gaujal &middot; Kimang Khun</div>
    <div class="author-str higher"></div>
    <div class="text-muted touchup-date-div" id="touchup-date-event-Sh3RF9JowK"></div>

    <a href="paper_pages/Sh3RF9JowK.html">
        <img src="http://img.youtube.com/vi/Ii2773_g3po/0.jpg" class="social-img-thumb rounded" alt="thumbnail"/>
    </a>

    <div class="abstract-section">
        <div>
            <a id="abstract-link-Sh3RF9JowK" class="abstract-link" data-toggle="collapse"
               href="#collapse-event-abstract-Sh3RF9JowK" role="button"
               aria-expanded="false" aria-controls="collapse-event-abstract-Sh3RF9JowK">
                Abstract <i id="caret-Sh3RF9JowK" class="fas fa-caret-right"></i>
            </a>
        </div>
    </div>

    <div class="collapse" id="collapse-event-abstract-Sh3RF9JowK">
        <div class="abstract-display">
            <p>In this paper, we study the scalability of model-based algorithms learning the optimal policy of a discounted \blue{rested} Markovian bandit  problem with $n$ arms. There are two categories of model-based reinforcement learning algorithms: Bayesian algorithms (like PSRL), and optimistic algorithms (like UCRL2 or UCBVI).  A naive application of these  algorithms is not scalable because  the state-space is exponential in $n$. In this paper, we construct variants of these algorithms specially tailored to Markovian bandits (MB) that we call MB-PSRL, MB-UCRL2, and MB-UCBVI. \blue{We consider an episodic setting with geometrically distributed episode length, and measure the performance of the algorithm in terms of regret (Bayesian regret for MB-PSRL and expected regret for MB-UCRL2 and MB-UCBVI)}. We prove that, for this setting, all algorithms have a low regret in $\tilde{O}(S\sqrt{nK})$ -- where $K$ is the number of episodes, $n$ is the number of arms and $S$ is the number of states of each arm. Up to a factor $\sqrt{S}$, these regrets match the \blue{Bayesian minimax regret} lower bound of $\Omega(\sqrt{SnK})$ that we also derive.

Even if their theoretical regrets are comparable, the {\it time complexities} of these  algorithms vary greatly: We show that MB-UCRL2, as well as all  algorithms that use bonuses on transition matrices have a { time} complexity  that grows  exponentially in $n$.  In contrast, MB-UCBVI does not use bonuses on transition matrices and we show that  it can be implemented efficiently, with a time complexity linear in $n$. Our numerical experiments show, however, that its empirical regret is large. Our Bayesian algorithm, MB-PSRL, enjoys the best of both worlds: its running time is linear in the number of arms and its empirical regret is the smallest of all algorithms.
This is a new addition in the understanding of the power of Bayesian algorithms, that can often be  tailored to the structure of the problems to learn.</p>
        </div>
    </div>
</div>

<div class="displaycards touchup-date" id="event-mbwm7NdkpO">
    <div style="width:80%;margin:auto;">
        <a class="small-title" href="paper_pages/mbwm7NdkpO.html">Deep Policies for Online Bipartite Matching: A Reinforcement Learning Approach</a>
    </div>
    <div class="type_display_name_minus_type"></div>
    <div class="author-str">Mohammad Ali Alomrani &middot; Reza Moravej &middot; Elias Boutros Khalil</div>
    <div class="author-str higher"></div>
    <div class="text-muted touchup-date-div" id="touchup-date-event-mbwm7NdkpO"></div>

    <a href="paper_pages/mbwm7NdkpO.html">
        <img src="http://img.youtube.com/vi/PoMrl5rjQ3U/0.jpg" class="social-img-thumb rounded" alt="thumbnail"/>
    </a>

    <div class="abstract-section">
        <div>
            <a id="abstract-link-mbwm7NdkpO" class="abstract-link" data-toggle="collapse"
               href="#collapse-event-abstract-mbwm7NdkpO" role="button"
               aria-expanded="false" aria-controls="collapse-event-abstract-mbwm7NdkpO">
                Abstract <i id="caret-mbwm7NdkpO" class="fas fa-caret-right"></i>
            </a>
        </div>
    </div>

    <div class="collapse" id="collapse-event-abstract-mbwm7NdkpO">
        <div class="abstract-display">
            <p>The challenge in the widely applicable online matching problem lies in making irrevocable assignments while there is uncertainty about future inputs. Most theoretically-grounded policies are myopic or greedy in nature. In real-world applications where the matching process is repeated on a regular basis, the underlying data distribution can be leveraged for better decision-making. We present an end-to-end Reinforcement Learning framework for deriving better matching policies based on trial-and-error on historical data. We devise a set of neural network architectures, design feature representations, and empirically evaluate them across two online matching problems: Edge-Weighted Online Bipartite Matching and Online Submodular Bipartite Matching. We show that most of the learning approaches perform consistently better than classical baseline algorithms on four synthetic and real-world datasets. On average, our proposed models improve the matching quality by 3-10% on a variety of synthetic and real-world datasets.
</p>
        </div>
    </div>
</div>

<div class="displaycards touchup-date" id="event-aIoEkwc2oB">
    <div style="width:80%;margin:auto;">
        <a class="small-title" href="paper_pages/aIoEkwc2oB.html">INR-V: A Continuous Representation Space for Video-based Generative Tasks</a>
    </div>
    <div class="type_display_name_minus_type"></div>
    <div class="author-str">Bipasha Sen &middot; Aditya Agarwal &middot; Vinay P Namboodiri &middot; C.V. Jawahar</div>
    <div class="author-str higher"></div>
    <div class="text-muted touchup-date-div" id="touchup-date-event-aIoEkwc2oB"></div>

    <a href="paper_pages/aIoEkwc2oB.html">
        <img src="tmlr_logo.jpeg" class="social-img-thumb rounded" alt="thumbnail"/>
    </a>

    <div class="abstract-section">
        <div>
            <a id="abstract-link-aIoEkwc2oB" class="abstract-link" data-toggle="collapse"
               href="#collapse-event-abstract-aIoEkwc2oB" role="button"
               aria-expanded="false" aria-controls="collapse-event-abstract-aIoEkwc2oB">
                Abstract <i id="caret-aIoEkwc2oB" class="fas fa-caret-right"></i>
            </a>
        </div>
    </div>

    <div class="collapse" id="collapse-event-abstract-aIoEkwc2oB">
        <div class="abstract-display">
            <p>Generating videos is a complex task that is accomplished by generating a set of temporally coherent images frame-by-frame. This limits the expressivity of videos to only image-based operations on the individual video frames needing network designs to obtain temporally coherent trajectories in the underlying image space. We propose INR-V, a video representation network that learns a continuous space for video-based generative tasks. INR-V parameterizes videos using implicit neural representations (INRs), a multi-layered perceptron that predicts an RGB value for each input pixel location of the video. The INR is predicted using a meta-network which is a hypernetwork trained on neural representations of multiple video instances. Later, the meta-network can be sampled to generate diverse novel videos enabling many downstream video-based generative tasks. Interestingly, we find that conditional regularization and progressive weight initialization play a crucial role in obtaining INR-V. The representation space learned by INR-V is more expressive than an image space showcasing many interesting properties not possible with the existing works. For instance, INR-V can smoothly interpolate intermediate videos between known video instances (such as intermediate identities, expressions, and poses in face videos). It can also in-paint missing portions in videos to recover temporally coherent full videos. In this work, we evaluate the space learned by INR-V on diverse generative tasks such as video interpolation, novel video generation, video inversion, and video inpainting against the existing baselines. INR-V significantly outperforms the baselines on several of these demonstrated tasks, clearly showing the potential of the proposed representation space.</p>
        </div>
    </div>
</div>

<div class="displaycards touchup-date" id="event-lCPOHiztuw">
    <div style="width:80%;margin:auto;">
        <a class="small-title" href="paper_pages/lCPOHiztuw.html">Direct Molecular Conformation Generation</a>
    </div>
    <div class="type_display_name_minus_type"></div>
    <div class="author-str">Jinhua Zhu &middot; Yingce Xia &middot; Chang Liu &middot; Lijun Wu &middot; Shufang Xie &middot; Yusong Wang &middot; Tong Wang &middot; Tao Qin &middot; Wengang Zhou &middot; Houqiang Li &middot; Haiguang Liu &middot; Tie-Yan Liu</div>
    <div class="author-str higher"></div>
    <div class="text-muted touchup-date-div" id="touchup-date-event-lCPOHiztuw"></div>

    <a href="paper_pages/lCPOHiztuw.html">
        <img src="https://drive.google.com/thumbnail?id=1p_-NHOWBH3oiPFuaJiskXYWGdmRmwtLK" class="social-img-thumb rounded" alt="thumbnail"/>
    </a>

    <div class="abstract-section">
        <div>
            <a id="abstract-link-lCPOHiztuw" class="abstract-link" data-toggle="collapse"
               href="#collapse-event-abstract-lCPOHiztuw" role="button"
               aria-expanded="false" aria-controls="collapse-event-abstract-lCPOHiztuw">
                Abstract <i id="caret-lCPOHiztuw" class="fas fa-caret-right"></i>
            </a>
        </div>
    </div>

    <div class="collapse" id="collapse-event-abstract-lCPOHiztuw">
        <div class="abstract-display">
            <p>Molecular conformation generation aims to generate three-dimensional coordinates of all the atoms in a molecule and is an important task in bioinformatics and pharmacology. Previous methods usually first predict the interatomic distances, the gradients of interatomic distances or the local structures (e.g., torsion angles) of a molecule, and then reconstruct its 3D conformation. How to directly generate the conformation without the above intermediate values is not fully explored. In this work, we propose a method that directly predicts the coordinates of atoms: (1) the loss function is invariant to roto-translation of coordinates and permutation of symmetric atoms; (2) the newly proposed model adaptively aggregates the bond and atom information and iteratively refines the coordinates of the generated conformation. Our method achieves the best results on GEOM-QM9 and GEOM-Drugs datasets. Further analysis shows that our generated conformations have closer properties (e.g., HOMO-LUMO gap) with the groundtruth conformations. In addition, our method improves molecular docking by providing better initial conformations. All the results demonstrate the effectiveness of our method and the great potential of the direct approach. The code is released at  \url{https://github.com/DirectMolecularConfGen/DMCG}.</p>
        </div>
    </div>
</div>

<div class="displaycards touchup-date" id="event-oRjk5V9eDp">
    <div style="width:80%;margin:auto;">
        <a class="small-title" href="paper_pages/oRjk5V9eDp.html">Bayesian Methods for Constraint Inference in Reinforcement Learning</a>
    </div>
    <div class="type_display_name_minus_type"></div>
    <div class="author-str">Dimitris Papadimitriou &middot; Usman Anwar &middot; Daniel S. Brown</div>
    <div class="author-str higher"></div>
    <div class="text-muted touchup-date-div" id="touchup-date-event-oRjk5V9eDp"></div>

    <a href="paper_pages/oRjk5V9eDp.html">
        <img src="https://drive.google.com/thumbnail?id=1_DMCjrmn5FygavUND1e71qJeNEK_Utch" class="social-img-thumb rounded" alt="thumbnail"/>
    </a>

    <div class="abstract-section">
        <div>
            <a id="abstract-link-oRjk5V9eDp" class="abstract-link" data-toggle="collapse"
               href="#collapse-event-abstract-oRjk5V9eDp" role="button"
               aria-expanded="false" aria-controls="collapse-event-abstract-oRjk5V9eDp">
                Abstract <i id="caret-oRjk5V9eDp" class="fas fa-caret-right"></i>
            </a>
        </div>
    </div>

    <div class="collapse" id="collapse-event-abstract-oRjk5V9eDp">
        <div class="abstract-display">
            <p>Learning constraints from demonstrations provides a natural and efficient way to improve the safety of AI systems; however, prior work only considers learning a single, point-estimate of the constraints. By contrast, we consider the problem of inferring constraints from demonstrations using a Bayesian perspective. We propose Bayesian Inverse Constraint Reinforcement Learning (BICRL), a novel approach that infers a posterior probability distribution over constraints from demonstrated trajectories. The main advantages of BICRL, compared to prior constraint inference algorithms, are (1) the freedom to infer constraints from partial trajectories and even from disjoint state-action pairs,  (2) the ability to infer constraints from suboptimal demonstrations and in stochastic environments, and (3) the opportunity to use the posterior distribution over constraints in order to implement active learning and robust policy optimization techniques. We show that BICRL outperforms pre-existing constraint learning approaches, leading to more accurate constraint inference and consequently safer policies. We further propose Hierarchical BICRL that infers constraints locally in sub-spaces of the entire domain and then composes global constraint estimates leading to accurate and computationally efficient constraint estimation.  </p>
        </div>
    </div>
</div>

<div class="displaycards touchup-date" id="event-6qMKztPn0n">
    <div style="width:80%;margin:auto;">
        <a class="small-title" href="paper_pages/6qMKztPn0n.html">Evolving Decomposed Plasticity Rules for Information-Bottlenecked Meta-Learning</a>
    </div>
    <div class="type_display_name_minus_type"></div>
    <div class="author-str">Fan Wang &middot; Hao Tian &middot; Haoyi Xiong &middot; Hua Wu &middot; Jie Fu &middot; Yang Cao &middot; Kang Yu &middot; Haifeng Wang</div>
    <div class="author-str higher"></div>
    <div class="text-muted touchup-date-div" id="touchup-date-event-6qMKztPn0n"></div>

    <a href="paper_pages/6qMKztPn0n.html">
        <img src="http://img.youtube.com/vi/8EA8KqlCRzY/0.jpg" class="social-img-thumb rounded" alt="thumbnail"/>
    </a>

    <div class="abstract-section">
        <div>
            <a id="abstract-link-6qMKztPn0n" class="abstract-link" data-toggle="collapse"
               href="#collapse-event-abstract-6qMKztPn0n" role="button"
               aria-expanded="false" aria-controls="collapse-event-abstract-6qMKztPn0n">
                Abstract <i id="caret-6qMKztPn0n" class="fas fa-caret-right"></i>
            </a>
        </div>
    </div>

    <div class="collapse" id="collapse-event-abstract-6qMKztPn0n">
        <div class="abstract-display">
            <p>Artificial neural networks (ANNs) are typically confined to accomplishing pre-defined tasks by learning a set of static parameters. In contrast, biological neural networks (BNNs) can adapt to various new tasks by continually updating the neural connections based on the inputs, which is aligned with the paradigm of learning effective learning rules in addition to static parameters, \textit{e.g.}, meta-learning. Among various biologically inspired learning rules, Hebbian plasticity updates the neural network weights using local signals without the guide of an explicit target function, thus enabling an agent to learn automatically without human efforts. However, typical plastic ANNs using a large amount of meta-parameters violate the nature of the genomics bottleneck and potentially deteriorate the generalization capacity. This work proposes a new learning paradigm decomposing those connection-dependent plasticity rules into neuron-dependent rules thus accommodating $\Theta(n^2)$ learnable parameters with only $\Theta(n)$ meta-parameters. We also thoroughly study the effect of different neural modulation on plasticity. Our algorithms are tested in challenging random 2D maze environments, where the agents have to use their past experiences to shape the neural connections and improve their performances for the future. The results of our experiment validate the following: 1. Plasticity can be adopted to continually update a randomly initialized RNN to surpass pre-trained, more sophisticated recurrent models, especially when coming to long-term memorization. 2. Following the genomics bottleneck, the proposed decomposed plasticity can be comparable to or even more effective than canonical plasticity rules in some instances.</p>
        </div>
    </div>
</div>

<div class="displaycards touchup-date" id="event-M8D5iZsnrO">
    <div style="width:80%;margin:auto;">
        <a class="small-title" href="paper_pages/M8D5iZsnrO.html">TITRATED: Learned Human Driving Behavior without Infractions via Amortized Inference</a>
    </div>
    <div class="type_display_name_minus_type"></div>
    <div class="author-str">Vasileios Lioutas &middot; Adam Scibior &middot; Frank Wood</div>
    <div class="author-str higher"></div>
    <div class="text-muted touchup-date-div" id="touchup-date-event-M8D5iZsnrO"></div>

    <a href="paper_pages/M8D5iZsnrO.html">
        <img src="http://img.youtube.com/vi/AMeZtzQxhX4/0.jpg" class="social-img-thumb rounded" alt="thumbnail"/>
    </a>

    <div class="abstract-section">
        <div>
            <a id="abstract-link-M8D5iZsnrO" class="abstract-link" data-toggle="collapse"
               href="#collapse-event-abstract-M8D5iZsnrO" role="button"
               aria-expanded="false" aria-controls="collapse-event-abstract-M8D5iZsnrO">
                Abstract <i id="caret-M8D5iZsnrO" class="fas fa-caret-right"></i>
            </a>
        </div>
    </div>

    <div class="collapse" id="collapse-event-abstract-M8D5iZsnrO">
        <div class="abstract-display">
            <p>Models of human driving behavior have long been used for prediction in autonomous vehicles, but recently have also started being used to create non-playable characters for driving simulations. While such models are in many respects realistic, they tend to suffer from unacceptably high rates of driving infractions, such as collisions or off-road driving, particularly when deployed in map locations with road geometries dissimilar to the training dataset. In this paper we present a novel method for fine-tuning a foundation model of human driving behavior to novel locations where human demonstrations are not available which reduces the incidence of such infractions. The method relies on inference in the foundation model to generate infraction-free trajectories as well as additional penalties applied when fine-tuning the amortized inference behavioral model. We demonstrate this "titration" technique using the ITRA foundation behavior model trained on the INTERACTION dataset when transferring to CARLA map locations. We demonstrate a 76-86% reduction in infraction rate and provide evidence that further gains are possible with more computation or better inference algorithms.</p>
        </div>
    </div>
</div>

<div class="displaycards touchup-date" id="event-lE7K4n1Esk">
    <div style="width:80%;margin:auto;">
        <a class="small-title" href="paper_pages/lE7K4n1Esk.html">On the Adversarial Robustness of Vision Transformers</a>
    </div>
    <div class="type_display_name_minus_type"></div>
    <div class="author-str">Rulin Shao &middot; Zhouxing Shi &middot; Jinfeng Yi &middot; Pin-Yu Chen &middot; Cho-Jui Hsieh</div>
    <div class="author-str higher"></div>
    <div class="text-muted touchup-date-div" id="touchup-date-event-lE7K4n1Esk"></div>

    <a href="paper_pages/lE7K4n1Esk.html">
        <img src="http://img.youtube.com/vi/paMjHR5ufRs/0.jpg" class="social-img-thumb rounded" alt="thumbnail"/>
    </a>

    <div class="abstract-section">
        <div>
            <a id="abstract-link-lE7K4n1Esk" class="abstract-link" data-toggle="collapse"
               href="#collapse-event-abstract-lE7K4n1Esk" role="button"
               aria-expanded="false" aria-controls="collapse-event-abstract-lE7K4n1Esk">
                Abstract <i id="caret-lE7K4n1Esk" class="fas fa-caret-right"></i>
            </a>
        </div>
    </div>

    <div class="collapse" id="collapse-event-abstract-lE7K4n1Esk">
        <div class="abstract-display">
            <p>Following the success in advancing natural language processing and understanding, transformers are expected to bring revolutionary changes to computer vision. This work provides a comprehensive study on the robustness of vision transformers (ViTs) against adversarial perturbations. Tested on various white-box and transfer attack settings, we find that ViTs possess better adversarial robustness when compared with MLP-Mixer and convolutional neural networks (CNNs) including ConvNeXt, and this observation also holds for certified robustness. Through frequency analysis and feature visualization, we summarize the following main observations contributing to the improved robustness of ViTs: 1) Features learned by ViTs contain less high-frequency patterns that have spurious correlation,  which helps explain why ViTs are less sensitive to high-frequency perturbations than CNNs and MLP-Mixer, and there is a high correlation between how much the model learns high-frequency features and its robustness against different frequency-based perturbations. 2) Introducing convolutional or tokens-to-token blocks for learning high-frequency features in ViTs can improve classification accuracy but at the cost of adversarial robustness. 3) Modern CNN designs that borrow techniques from ViTs including activation function, layer norm, larger kernel size to imitate the global attention, and patchify the images as inputs, etc., could help bridge the performance gap between ViTs and CNNs not only in terms of performance, but also certified and empirical adversarial robustness. Moreover, we show adversarial training is also applicable to ViT for training robust models, and sharpness-aware minimization can also help improve robustness, while pre-training with clean images on larger datasets does not significantly improve adversarial robustness. </p>
        </div>
    </div>
</div>

<div class="displaycards touchup-date" id="event-XsPopigZXV">
    <div style="width:80%;margin:auto;">
        <a class="small-title" href="paper_pages/XsPopigZXV.html">FLEA: Provably Robust Fair Multisource Learning from Unreliable Training Data</a>
    </div>
    <div class="type_display_name_minus_type"></div>
    <div class="author-str">Eugenia Iofinova &middot; Nikola Konstantinov &middot; Christoph H Lampert</div>
    <div class="author-str higher"></div>
    <div class="text-muted touchup-date-div" id="touchup-date-event-XsPopigZXV"></div>

    <a href="paper_pages/XsPopigZXV.html">
        <img src="tmlr_logo.jpeg" class="social-img-thumb rounded" alt="thumbnail"/>
    </a>

    <div class="abstract-section">
        <div>
            <a id="abstract-link-XsPopigZXV" class="abstract-link" data-toggle="collapse"
               href="#collapse-event-abstract-XsPopigZXV" role="button"
               aria-expanded="false" aria-controls="collapse-event-abstract-XsPopigZXV">
                Abstract <i id="caret-XsPopigZXV" class="fas fa-caret-right"></i>
            </a>
        </div>
    </div>

    <div class="collapse" id="collapse-event-abstract-XsPopigZXV">
        <div class="abstract-display">
            <p>Fairness-aware learning aims at constructing classifiers that not only make accurate predictions, but also do not discriminate against specific groups. It is a fast-growing area of machine learning with far-reaching societal impact. However, existing fair learning methods are vulnerable to accidental or malicious artifacts in the training data, which can cause them to unknowingly produce unfair classifiers. In this work we address the problem of fair learning from unreliable training data in the robust multisource setting, where the available training data comes from multiple sources, a fraction of which might not be representative of the true data distribution. We introduce FLEA, a filtering-based algorithm that identifies and suppresses those data sources that would have a negative impact on fairness or accuracy if they were used for training. As such, FLEA is not a replacement of prior fairness-aware learning methods but rather an augmentation that makes any of them robust against unreliable training data. We show the effectiveness of our approach by a diverse range of experiments on multiple datasets. Additionally, we prove formally that –given enough data– FLEA protects the learner against corruptions as long as the fraction of affected data sources is less than half. Our source code and documentation are available at https://github.com/ISTAustria-CVML/FLEA.</p>
        </div>
    </div>
</div>

<div class="displaycards touchup-date" id="event-gCmQK6McbR">
    <div style="width:80%;margin:auto;">
        <a class="small-title" href="paper_pages/gCmQK6McbR.html">HEAT: Hyperedge Attention Networks</a>
    </div>
    <div class="type_display_name_minus_type"></div>
    <div class="author-str">Dobrik Georgiev Georgiev &middot; Marc Brockschmidt &middot; Miltiadis Allamanis</div>
    <div class="author-str higher"></div>
    <div class="text-muted touchup-date-div" id="touchup-date-event-gCmQK6McbR"></div>

    <a href="paper_pages/gCmQK6McbR.html">
        <img src="http://img.youtube.com/vi/q0oFjmxz_60/0.jpg" class="social-img-thumb rounded" alt="thumbnail"/>
    </a>

    <div class="abstract-section">
        <div>
            <a id="abstract-link-gCmQK6McbR" class="abstract-link" data-toggle="collapse"
               href="#collapse-event-abstract-gCmQK6McbR" role="button"
               aria-expanded="false" aria-controls="collapse-event-abstract-gCmQK6McbR">
                Abstract <i id="caret-gCmQK6McbR" class="fas fa-caret-right"></i>
            </a>
        </div>
    </div>

    <div class="collapse" id="collapse-event-abstract-gCmQK6McbR">
        <div class="abstract-display">
            <p>Learning from structured data is a core machine learning task. Commonly, such data is represented as graphs, which normally only consider (typed) binary relationships between pairs of nodes. This is a substantial limitation for many domains with highly-structured data. One important such domain is source code, where hypergraph-based representations can better capture the semantically rich and structured nature of code.
In this work, we present HEAT, a neural model capable of representing typed and qualified hypergraphs, where each hyperedge explicitly qualifies how participating nodes contribute. It can be viewed as a generalization of both message passing neural networks and Transformers. We evaluate HEAT on knowledge base completion and on bug detection and repair using a novel hypergraph representation of programs. In both settings, it outperforms strong baselines, indicating its power and generality.</p>
        </div>
    </div>
</div>

<div class="displaycards touchup-date" id="event-e7mYYMSyZH">
    <div style="width:80%;margin:auto;">
        <a class="small-title" href="paper_pages/e7mYYMSyZH.html">On the Convergence of Shallow Neural Network Training with Randomly Masked Neurons</a>
    </div>
    <div class="type_display_name_minus_type"></div>
    <div class="author-str">Fangshuo Liao &middot; Anastasios Kyrillidis</div>
    <div class="author-str higher"></div>
    <div class="text-muted touchup-date-div" id="touchup-date-event-e7mYYMSyZH"></div>

    <a href="paper_pages/e7mYYMSyZH.html">
        <img src="http://img.youtube.com/vi/8dR4tcfOTkE/0.jpg" class="social-img-thumb rounded" alt="thumbnail"/>
    </a>

    <div class="abstract-section">
        <div>
            <a id="abstract-link-e7mYYMSyZH" class="abstract-link" data-toggle="collapse"
               href="#collapse-event-abstract-e7mYYMSyZH" role="button"
               aria-expanded="false" aria-controls="collapse-event-abstract-e7mYYMSyZH">
                Abstract <i id="caret-e7mYYMSyZH" class="fas fa-caret-right"></i>
            </a>
        </div>
    </div>

    <div class="collapse" id="collapse-event-abstract-e7mYYMSyZH">
        <div class="abstract-display">
            <p>With the motive of training all the parameters of a neural network, we study why and when one can achieve this by iteratively creating, training, and combining randomly selected subnetworks. Such scenarios have either implicitly or explicitly emerged in the recent literature: see e.g., the Dropout family of regularization techniques, or some distributed ML training protocols that reduce communication/computation complexities, such as the Independent Subnet Training protocol. While these methods are studied empirically and utilized in practice, they often enjoy partial or no theoretical support, especially when applied on neural network-based objectives.

In this manuscript, our focus is on overparameterized single hidden layer neural networks with ReLU activations in the lazy training regime. By carefully analyzing $i)$ the subnetworks' neural tangent kernel, $ii)$ the surrogate functions' gradient, and $iii)$ how we sample and combine the surrogate functions, we prove linear convergence rate of the training error --up to a neighborhood around the optimal point-- for an overparameterized single-hidden layer perceptron with a regression loss. Our analysis reveals a dependency of the size of the neighborhood around the optimal point on the number of surrogate models and the number of local training steps for each selected subnetwork. Moreover, the considered framework generalizes and provides new insights on dropout training, multi-sample dropout training, as well as Independent Subnet Training; for each case, we provide convergence results as corollaries of our main theorem.</p>
        </div>
    </div>
</div>

<div class="displaycards touchup-date" id="event-qzM1Tw5i7N">
    <div style="width:80%;margin:auto;">
        <a class="small-title" href="paper_pages/qzM1Tw5i7N.html">SemiNLL: A Framework of Noisy-Label Learning by Semi-Supervised Learning</a>
    </div>
    <div class="type_display_name_minus_type"></div>
    <div class="author-str">ZHUOWEI WANG &middot; Jing Jiang &middot; Bo Han &middot; Lei Feng &middot; Bo An &middot; Gang Niu &middot; Guodong Long</div>
    <div class="author-str higher"></div>
    <div class="text-muted touchup-date-div" id="touchup-date-event-qzM1Tw5i7N"></div>

    <a href="paper_pages/qzM1Tw5i7N.html">
        <img src="http://img.youtube.com/vi/sKDRt9GNLTs/0.jpg" class="social-img-thumb rounded" alt="thumbnail"/>
    </a>

    <div class="abstract-section">
        <div>
            <a id="abstract-link-qzM1Tw5i7N" class="abstract-link" data-toggle="collapse"
               href="#collapse-event-abstract-qzM1Tw5i7N" role="button"
               aria-expanded="false" aria-controls="collapse-event-abstract-qzM1Tw5i7N">
                Abstract <i id="caret-qzM1Tw5i7N" class="fas fa-caret-right"></i>
            </a>
        </div>
    </div>

    <div class="collapse" id="collapse-event-abstract-qzM1Tw5i7N">
        <div class="abstract-display">
            <p>Deep learning with noisy labels is a challenging task, which has received much attention from the machine learning and computer vision communities. Recent prominent methods that build on a specific sample selection (SS) strategy and a specific semi-supervised learning (SSL) model achieved state-of-the-art performance. Intuitively, better performance could be achieved if stronger SS strategies and SSL models are employed. Following this intuition, one might easily derive various effective noisy-label learning methods using different combinations of SS strategies and SSL models, which is, however, simply reinventing the wheel in essence. To prevent this problem, we propose SemiNLL, a versatile framework that investigates how to naturally combine different SS and SSL components based on their effects and efficiencies. We conduct a systematic and detailed analysis of the combinations of possible components based on our framework. Our framework can absorb various SS strategies and SSL backbones, utilizing their power to achieve promising performance. The instantiations of our framework demonstrate substantial improvements over state-of-the-art methods on benchmark-simulated and real-world datasets with noisy labels.
</p>
        </div>
    </div>
</div>

<div class="displaycards touchup-date" id="event-0nEZCVshxS">
    <div style="width:80%;margin:auto;">
        <a class="small-title" href="paper_pages/0nEZCVshxS.html">Diagnosing and Fixing Manifold Overfitting in Deep Generative Models</a>
    </div>
    <div class="type_display_name_minus_type"></div>
    <div class="author-str">Gabriel Loaiza-Ganem &middot; Brendan Leigh Ross &middot; Jesse C Cresswell &middot; Anthony L. Caterini</div>
    <div class="author-str higher"></div>
    <div class="text-muted touchup-date-div" id="touchup-date-event-0nEZCVshxS"></div>

    <a href="paper_pages/0nEZCVshxS.html">
        <img src="http://img.youtube.com/vi/-2YaUfMlwrU/0.jpg" class="social-img-thumb rounded" alt="thumbnail"/>
    </a>

    <div class="abstract-section">
        <div>
            <a id="abstract-link-0nEZCVshxS" class="abstract-link" data-toggle="collapse"
               href="#collapse-event-abstract-0nEZCVshxS" role="button"
               aria-expanded="false" aria-controls="collapse-event-abstract-0nEZCVshxS">
                Abstract <i id="caret-0nEZCVshxS" class="fas fa-caret-right"></i>
            </a>
        </div>
    </div>

    <div class="collapse" id="collapse-event-abstract-0nEZCVshxS">
        <div class="abstract-display">
            <p>Likelihood-based, or explicit, deep generative models use neural networks to construct flexible high-dimensional densities. This formulation directly contradicts the manifold hypothesis, which states that observed data lies on a low-dimensional manifold embedded in high-dimensional ambient space. In this paper we investigate the pathologies of maximum-likelihood training in the presence of this dimensionality mismatch. We formally prove that degenerate optima are achieved wherein the manifold itself is learned but not the distribution on it, a phenomenon we call manifold overfitting. We propose a class of two-step procedures consisting of a dimensionality reduction step followed by maximum-likelihood density estimation, and prove that they recover the data-generating distribution in the nonparametric regime, thus avoiding manifold overfitting. We also show that these procedures enable density estimation on the manifolds learned by implicit models, such as generative adversarial networks, hence addressing a major shortcoming of these models. Several recently proposed methods are instances of our two-step procedures; we thus unify, extend, and theoretically justify a large class of models.</p>
        </div>
    </div>
</div>

<div class="displaycards touchup-date" id="event-8GvRCWKHIL">
    <div style="width:80%;margin:auto;">
        <a class="small-title" href="paper_pages/8GvRCWKHIL.html">Optimal Client Sampling for Federated Learning</a>
    </div>
    <div class="type_display_name_minus_type"></div>
    <div class="author-str">Wenlin Chen &middot; Samuel Horváth &middot; Peter Richtárik</div>
    <div class="author-str higher"></div>
    <div class="text-muted touchup-date-div" id="touchup-date-event-8GvRCWKHIL"></div>

    <a href="paper_pages/8GvRCWKHIL.html">
        <img src="http://img.youtube.com/vi/lhLJL1FJ_OE/0.jpg" class="social-img-thumb rounded" alt="thumbnail"/>
    </a>

    <div class="abstract-section">
        <div>
            <a id="abstract-link-8GvRCWKHIL" class="abstract-link" data-toggle="collapse"
               href="#collapse-event-abstract-8GvRCWKHIL" role="button"
               aria-expanded="false" aria-controls="collapse-event-abstract-8GvRCWKHIL">
                Abstract <i id="caret-8GvRCWKHIL" class="fas fa-caret-right"></i>
            </a>
        </div>
    </div>

    <div class="collapse" id="collapse-event-abstract-8GvRCWKHIL">
        <div class="abstract-display">
            <p>It is well understood that client-master communication can be a primary bottleneck in federated learning (FL). In this work, we address this issue with a novel client subsampling scheme, where we restrict the number of clients allowed to communicate their updates back to the master node. In each communication round, all participating clients compute their updates, but only the ones with "important" updates communicate back to the master. We show that importance can be measured using only the norm of the update and give a formula for optimal client participation. This formula minimizes the distance between the full update, where all clients participate, and our limited update, where the number of participating clients is restricted. In addition, we provide a simple algorithm that approximates the optimal formula for client participation, which allows for secure aggregation and stateless clients, and thus does not compromise client privacy. We show both theoretically and empirically that for Distributed SGD (DSGD) and Federated Averaging (FedAvg), the performance of our approach can be close to full participation and superior to the baseline where participating clients are sampled uniformly. Moreover, our approach is orthogonal to and compatible with existing methods for reducing communication overhead, such as local methods and communication compression methods. </p>
        </div>
    </div>
</div>

<div class="displaycards touchup-date" id="event-2EOVIvRXlv">
    <div style="width:80%;margin:auto;">
        <a class="small-title" href="paper_pages/2EOVIvRXlv.html">Ranking Recovery under Privacy Considerations</a>
    </div>
    <div class="type_display_name_minus_type"></div>
    <div class="author-str">Minoh Jeong &middot; Alex Dytso &middot; Martina Cardone</div>
    <div class="author-str higher"></div>
    <div class="text-muted touchup-date-div" id="touchup-date-event-2EOVIvRXlv"></div>

    <a href="paper_pages/2EOVIvRXlv.html">
        <img src="tmlr_logo.jpeg" class="social-img-thumb rounded" alt="thumbnail"/>
    </a>

    <div class="abstract-section">
        <div>
            <a id="abstract-link-2EOVIvRXlv" class="abstract-link" data-toggle="collapse"
               href="#collapse-event-abstract-2EOVIvRXlv" role="button"
               aria-expanded="false" aria-controls="collapse-event-abstract-2EOVIvRXlv">
                Abstract <i id="caret-2EOVIvRXlv" class="fas fa-caret-right"></i>
            </a>
        </div>
    </div>

    <div class="collapse" id="collapse-event-abstract-2EOVIvRXlv">
        <div class="abstract-display">
            <p>We consider the private ranking recovery problem, where a data collector seeks to estimate the permutation/ranking of a data vector given a randomized (privatized) version of it. We aim to establish fundamental trade-offs between the performance of the estimation task, measured in terms of probability of error, and the level of privacy that can be guaranteed when the noise mechanism consists of adding artificial noise. Towards this end, we show the optimality of a low-complexity decision rule (referred to as linear decoder) for the estimation task, under several noise distributions widely used in the privacy literature (e.g., Gaussian, Laplace, and generalized normal model). We derive the Taylor series of the probability of error, which yields its first and second-order approximations when such a linear decoder is employed.  We quantify the guaranteed level of privacy using differential privacy (DP) types of metrics, such as $\epsilon$-DP and $(\alpha,\epsilon)$-Rényi DP. Finally, we put together the results to characterize trade-offs between privacy and probability of error.</p>
        </div>
    </div>
</div>

<div class="displaycards touchup-date" id="event-aRsLetumx1">
    <div style="width:80%;margin:auto;">
        <a class="small-title" href="paper_pages/aRsLetumx1.html">How Expressive are Transformers in Spectral Domain for Graphs?</a>
    </div>
    <div class="type_display_name_minus_type"></div>
    <div class="author-str">Anson Bastos &middot; Abhishek Nadgeri &middot; Kuldeep Singh &middot; Hiroki Kanezashi &middot; Toyotaro Suzumura &middot; Isaiah Onando Mulang'</div>
    <div class="author-str higher"></div>
    <div class="text-muted touchup-date-div" id="touchup-date-event-aRsLetumx1"></div>

    <a href="paper_pages/aRsLetumx1.html">
        <img src="http://img.youtube.com/vi/7JNDYQuRSas/0.jpg" class="social-img-thumb rounded" alt="thumbnail"/>
    </a>

    <div class="abstract-section">
        <div>
            <a id="abstract-link-aRsLetumx1" class="abstract-link" data-toggle="collapse"
               href="#collapse-event-abstract-aRsLetumx1" role="button"
               aria-expanded="false" aria-controls="collapse-event-abstract-aRsLetumx1">
                Abstract <i id="caret-aRsLetumx1" class="fas fa-caret-right"></i>
            </a>
        </div>
    </div>

    <div class="collapse" id="collapse-event-abstract-aRsLetumx1">
        <div class="abstract-display">
            <p>The recent works proposing transformer-based models for graphs have proven the inadequacy of Vanilla Transformer for graph representation learning. To understand this inadequacy, there is a need to investigate if spectral analysis of the transformer will reveal insights into its expressive power. Similar studies already established that spectral analysis of Graph neural networks (GNNs) provides extra perspectives on their expressiveness. 
In this work, we systematically study and establish the link between the spatial and spectral domain in the realm of the transformer. We further provide a theoretical analysis that the spatial attention mechanism in the transformer cannot effectively capture the desired frequency response, thus, inherently limiting its expressiveness in spectral space. Therefore, we propose FeTA, a framework that aims to perform attention over the entire graph spectrum (i.e. actual frequency components of the graph) analogous to the attention in spatial space. 
Empirical results suggest that FeTA provides homogeneous performance gain against vanilla transformer across all tasks on standard benchmarks and can easily be extended to GNN-based models with low-pass characteristics (e.g., GAT). </p>
        </div>
    </div>
</div>

<div class="displaycards touchup-date" id="event-h1zuM6cXpH">
    <div style="width:80%;margin:auto;">
        <a class="small-title" href="paper_pages/h1zuM6cXpH.html">Zero-Shot Learning with Common Sense Knowledge Graphs</a>
    </div>
    <div class="type_display_name_minus_type"></div>
    <div class="author-str">Nihal V. Nayak &middot; Stephen Bach</div>
    <div class="author-str higher"></div>
    <div class="text-muted touchup-date-div" id="touchup-date-event-h1zuM6cXpH"></div>

    <a href="paper_pages/h1zuM6cXpH.html">
        <img src="http://img.youtube.com/vi/VXt3MucMWvY/0.jpg" class="social-img-thumb rounded" alt="thumbnail"/>
    </a>

    <div class="abstract-section">
        <div>
            <a id="abstract-link-h1zuM6cXpH" class="abstract-link" data-toggle="collapse"
               href="#collapse-event-abstract-h1zuM6cXpH" role="button"
               aria-expanded="false" aria-controls="collapse-event-abstract-h1zuM6cXpH">
                Abstract <i id="caret-h1zuM6cXpH" class="fas fa-caret-right"></i>
            </a>
        </div>
    </div>

    <div class="collapse" id="collapse-event-abstract-h1zuM6cXpH">
        <div class="abstract-display">
            <p>Zero-shot learning relies on semantic class representations such as hand-engineered attributes or learned embeddings to predict classes without any labeled examples. We propose to learn class representations by embedding nodes from common sense knowledge graphs in a vector space. Common sense knowledge graphs are an untapped source of explicit high-level knowledge that requires little human effort to apply to a range of tasks. To capture the knowledge in the graph, we introduce ZSL-KG, a general-purpose framework with a novel transformer graph convolutional network (TrGCN) for generating class representations. Our proposed TrGCN architecture computes non-linear combinations of node neighbourhoods. Our results show that ZSL-KG improves over existing WordNet-based methods on five out of six zero-shot benchmark datasets in language and vision.</p>
        </div>
    </div>
</div>

<div class="displaycards touchup-date" id="event-xyt4wfdo4J">
    <div style="width:80%;margin:auto;">
        <a class="small-title" href="paper_pages/xyt4wfdo4J.html">Iterative State Estimation in Non-linear Dynamical Systems Using Approximate Expectation Propagation</a>
    </div>
    <div class="type_display_name_minus_type"></div>
    <div class="author-str">Sanket Kamthe &middot; So Takao &middot; Shakir Mohamed &middot; Marc Peter Deisenroth</div>
    <div class="author-str higher"></div>
    <div class="text-muted touchup-date-div" id="touchup-date-event-xyt4wfdo4J"></div>

    <a href="paper_pages/xyt4wfdo4J.html">
        <img src="http://img.youtube.com/vi/FrTY04gXErY/0.jpg" class="social-img-thumb rounded" alt="thumbnail"/>
    </a>

    <div class="abstract-section">
        <div>
            <a id="abstract-link-xyt4wfdo4J" class="abstract-link" data-toggle="collapse"
               href="#collapse-event-abstract-xyt4wfdo4J" role="button"
               aria-expanded="false" aria-controls="collapse-event-abstract-xyt4wfdo4J">
                Abstract <i id="caret-xyt4wfdo4J" class="fas fa-caret-right"></i>
            </a>
        </div>
    </div>

    <div class="collapse" id="collapse-event-abstract-xyt4wfdo4J">
        <div class="abstract-display">
            <p>Bayesian inference in non-linear dynamical systems seeks to find good posterior approximations of a latent state given a sequence of observations. Gaussian filters and smoothers, including the (extended/unscented) Kalman filter/smoother, which are commonly used in engineering applications, yield Gaussian posteriors on the latent state. While they are computationally efficient, they are often criticised for their crude approximation of the posterior state distribution. In this paper, we address this criticism by proposing a message passing scheme for iterative state estimation in non-linear dynamical systems, which yields more informative (Gaussian) posteriors on the latent states.  Our message passing scheme is based on expectation propagation (EP). We prove that classical Rauch--Tung--Striebel (RTS) smoothers, such as the extended Kalman smoother (EKS) or the unscented Kalman smoother (UKS), are special cases of our message passing scheme. Running the message passing scheme more than once can lead to significant improvements of the classical RTS smoothers, so that more informative state estimates can be obtained. We address potential convergence issues of EP by generalising our state estimation framework to damped updates and the consideration of general $\alpha$-divergences.</p>
        </div>
    </div>
</div>

<div class="displaycards touchup-date" id="event-ggPhsYCsm9">
    <div style="width:80%;margin:auto;">
        <a class="small-title" href="paper_pages/ggPhsYCsm9.html">NeSF: Neural Semantic Fields for Generalizable Semantic Segmentation of 3D Scenes</a>
    </div>
    <div class="type_display_name_minus_type"></div>
    <div class="author-str">Suhani Vora &middot; Noha Radwan &middot; Klaus Greff &middot; Henning Meyer &middot; Kyle Genova &middot; Mehdi S. M. Sajjadi &middot; Etienne Pot &middot; Andrea Tagliasacchi &middot; Daniel Duckworth</div>
    <div class="author-str higher"></div>
    <div class="text-muted touchup-date-div" id="touchup-date-event-ggPhsYCsm9"></div>

    <a href="paper_pages/ggPhsYCsm9.html">
        <img src="http://img.youtube.com/vi/odg-L-fG7Wo/0.jpg" class="social-img-thumb rounded" alt="thumbnail"/>
    </a>

    <div class="abstract-section">
        <div>
            <a id="abstract-link-ggPhsYCsm9" class="abstract-link" data-toggle="collapse"
               href="#collapse-event-abstract-ggPhsYCsm9" role="button"
               aria-expanded="false" aria-controls="collapse-event-abstract-ggPhsYCsm9">
                Abstract <i id="caret-ggPhsYCsm9" class="fas fa-caret-right"></i>
            </a>
        </div>
    </div>

    <div class="collapse" id="collapse-event-abstract-ggPhsYCsm9">
        <div class="abstract-display">
            <p>We present NeSF, a method for producing 3D semantic fields from posed RGB images alone. In place of classical 3D representations, our method builds on recent work in neural fields wherein 3D structure is captured by point-wise functions. We leverage this methodology to recover 3D density fields upon which we then train a 3D semantic segmentation model supervised by posed 2D semantic maps. Despite being trained on 2D signals alone, our method is able to generate 3D-consistent semantic maps from novel camera poses and can be queried at arbitrary 3D points. Notably, NeSF is compatible with any method producing a density field. Our empirical analysis demonstrates comparable quality to competitive 2D and 3D semantic segmentation baselines on complex, realistically-rendered scenes and significantly outperforms a comparable neural radiance field-based method on a series of tasks requiring 3D reasoning. Our method is the first to learn semantics by recognizing patterns in the geometry stored within a 3D neural field representation. NeSF is trained using purely 2D signals and requires as few as one labeled image per-scene at train time. No semantic input is required for inference on novel scenes.</p>
        </div>
    </div>
</div>

<div class="displaycards touchup-date" id="event-tnPjQpYk7D">
    <div style="width:80%;margin:auto;">
        <a class="small-title" href="paper_pages/tnPjQpYk7D.html">Multi-Agent Off-Policy TDC with Near-Optimal Sample and Communication Complexities</a>
    </div>
    <div class="type_display_name_minus_type"></div>
    <div class="author-str">Ziyi Chen &middot; Yi Zhou &middot; Rong-Rong Chen</div>
    <div class="author-str higher"></div>
    <div class="text-muted touchup-date-div" id="touchup-date-event-tnPjQpYk7D"></div>

    <a href="paper_pages/tnPjQpYk7D.html">
        <img src="http://img.youtube.com/vi/BCYEaLGtTNE/0.jpg" class="social-img-thumb rounded" alt="thumbnail"/>
    </a>

    <div class="abstract-section">
        <div>
            <a id="abstract-link-tnPjQpYk7D" class="abstract-link" data-toggle="collapse"
               href="#collapse-event-abstract-tnPjQpYk7D" role="button"
               aria-expanded="false" aria-controls="collapse-event-abstract-tnPjQpYk7D">
                Abstract <i id="caret-tnPjQpYk7D" class="fas fa-caret-right"></i>
            </a>
        </div>
    </div>

    <div class="collapse" id="collapse-event-abstract-tnPjQpYk7D">
        <div class="abstract-display">
            <p>The finite-time convergence of off-policy temporal difference (TD) learning has been comprehensively studied recently. However, such a type of convergence has not been established for off-policy TD learning in the multi-agent setting, which covers broader reinforcement learning applications and is fundamentally more challenging. This work develops a decentralized TD with correction (TDC) algorithm for multi-agent off-policy TD learning under Markovian sampling. In particular, our algorithm avoids sharing the actions, policies and rewards of the agents, and adopts mini-batch sampling to reduce the sampling variance and communication frequency. Under Markovian sampling and linear function approximation, we proved that the finite-time sample complexity of our algorithm for achieving an $\epsilon$-accurate solution is in the order of $\mathcal{O}\big(\frac{M\ln\epsilon^{-1}}{\epsilon(1-\sigma_2)^2}\big)$, where $M$ denotes the total number of agents and $\sigma_2$ is a network parameter. This matches the sample complexity of the centralized TDC. Moreover, our algorithm achieves the optimal communication complexity $\mathcal{O}\big(\frac{\sqrt{M}\ln\epsilon^{-1}}{1-\sigma_2}\big)$ for synchronizing the value function parameters, which is order-wise lower than the communication complexity of the existing decentralized TD(0). Numerical simulations corroborate our theoretical findings. </p>
        </div>
    </div>
</div>
                    </div>
                </div>
            </div>

        <script>
            function listmode(){
                $(".cards_img").hide();
                $(".pp-card").addClass("pp-mode-list").removeClass("pp-mode-compact");
            }
            function compactmode(){
                $(".cards_img").show();
                $(".pp-card").removeClass("pp-mode-list").addClass("pp-mode-compact");
            }
        </script>


<script>
    $(document).ready(function() {
        $(".abstract-link").on('click', function(e){
            var target = $(e.target).find("i")
            target.toggleClass("fa-caret-right");
            target.toggleClass("fa-caret-up");
        })
        touchup();
        /* touchup events currently adds dates to cached virtualcards for events */
    })

</script>


<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
  "tex2jax": {
    "inlineMath": [["$","$"], ["\(","\)"]],
    "displayMath": [["\[","\]"]],
    "processEscapes": true
  }
}
);
var jq2 = $;
</script>

<script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

</body>
</html>